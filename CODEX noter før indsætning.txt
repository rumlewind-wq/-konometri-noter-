Chapter 16 
Based on the new images provided, here is the reproduction of Chapter 16: Simultaneous Equations Models.
16 Simultaneous Equations Models
* Introduction:
o Previous chapters addressed endogeneity via omitted variables and measurement error.
o Simultaneity: A new form of endogeneity where one or more explanatory variables are jointly determined with the dependent variable, typically through an equilibrium mechanism.
o The leading estimation method is Instrumental Variables (IV).
16-1 The Nature of Simultaneous Equations Models
* Concept:
o Each equation in the system should have a ceteris paribus, causal interpretation.
o Because we observe equilibrium outcomes, we must use counterfactual reasoning to construct the model.
* Classic Example: Labor Supply and Demand:
o Labor Supply Function:
$$h_s = \alpha_1 w + \beta_1 z_1 + u_1$$
* $h_s$: hours supplied.
* $w$: wage.
* $z_1$: observed supply shifter (e.g., manufacturing wage in the county).
* $u_1$: unobserved supply shifter.
o Labor Demand Function:
$$h_d = \alpha_2 w + \beta_2 z_2 + u_2$$
* $h_d$: hours demanded.
* $z_2$: observed demand shifter (e.g., agricultural land area).
* $u_2$: unobserved demand shifter.
o Equilibrium Condition:
$$h_s = h_d$$
o Observation: We observe equilibrium hours $h_i$ and wages $w_i$ for each county $i$, determined by the intersection of supply and demand.
* Structural Model Structure:
o Combining the equilibrium condition with the supply and demand equations yields a Simultaneous Equations Model (SEM):
$$h_i = \alpha_1 w_i + \beta_1 z_{i1} + u_{i1}$$
$$h_i = \alpha_2 w_i + \beta_2 z_{i2} + u_{i2}$$
o Structural Equations: These equations are derived from economic theory and have causal interpretations.
o Endogenous Variables: $h_i$ and $w_i$ are jointly determined within the system.
o Exogenous Variables: $z_{i1}$ and $z_{i2}$ are determined outside the model and are uncorrelated with structural errors $u_{i1}$ and $u_{i2}$.
o Structural Errors: $u_{i1}$ and $u_{i2}$ appear in the structural equations.
* Autonomy Requirement:
o Each equation must stand on its own with a ceteris paribus interpretation.
o If equations are indistinguishable (e.g., supply and demand depend on the exact same variables), the parameters lose economic meaning.
* Example 16.1 (Murder Rates and Size of the Police Force):
o Goal: Determine the effect of police force size ($polpc$) on murder rates ($murdpc$).
o Structural Equation 1 (Crime Function):
$$murdpc = \alpha_1 polpc + \beta_{10} + \beta_{11} incpc + u_1$$
* We expect $\alpha_1 < 0$.
o Simultaneity Problem: Cities do not exogenously choose police force size. High crime rates lead to larger police forces.
o Structural Equation 2 (Police Response):
$$polpc = \alpha_2 murdpc + \beta_{20} + \text{other factors}$$
* We expect $\alpha_2 > 0$ (cities hire more police in response to crime).
o This feedback loop makes $polpc$ and $u_1$ correlated, rendering OLS biased.
* Example 16.2 (Housing Expenditures and Saving):
o Model:
$$housing = \alpha_1 saving + \beta_{10} + ... + u_1$$
$$saving = \alpha_2 housing + \beta_{20} + ... + u_2$$
o Critique: This system fails the autonomy requirement. $housing$ and $saving$ are chosen by the same household under a budget constraint.
o Holding saving fixed while changing income to see the effect on housing makes no sense if households optimize choices jointly.
o It is better to estimate reduced forms (demand functions) depending on income and prices, rather than an SEM.
16-2 Simultaneity Bias in OLS
* The Model: Consider a two-equation structural model:
$$y_1 = \alpha_1 y_2 + \beta_1 z_1 + u_1$$
$$y_2 = \alpha_2 y_1 + \beta_2 z_2 + u_2$$
o $z_1, z_2$ are exogenous (uncorrelated with $u_1, u_2$).
o Focus on estimating the first equation.
* Deriving the Bias:
o To see the correlation between $y_2$ and $u_1$, substitute the first equation into the second and solve for $y_2$:
$$y_2 = \alpha_2 (\alpha_1 y_2 + \beta_1 z_1 + u_1) + \beta_2 z_2 + u_2$$
o Assuming $\alpha_2 \alpha_1 \neq 1$, we derive the Reduced Form Equation for $y_2$:
$$y_2 = \pi_{21} z_1 + \pi_{22} z_2 + v_2$$
o Reduced Form Parameters: $\pi_{21} = \alpha_2 \beta_1 / (1 - \alpha_2 \alpha_1)$ and $\pi_{22} = \beta_2 / (1 - \alpha_2 \alpha_1)$ are nonlinear functions of structural parameters.
o Reduced Form Error ($v_2$):
$$v_2 = \frac{\alpha_2 u_1 + u_2}{1 - \alpha_2 \alpha_1}$$
* Covariance Condition:
o Because $v_2$ depends on $u_1$, $y_2$ depends on $u_1$.
o Assuming $u_1$ and $u_2$ are uncorrelated (for simplicity), the covariance is:
$$Cov(y_2, u_1) = Cov(v_2, u_1) = \frac{\alpha_2}{1 - \alpha_2 \alpha_1} E(u_1^2) = \frac{\alpha_2}{1 - \alpha_2 \alpha_1} \sigma_1^2$$
o Result: If $\alpha_2 \neq 0$ (feedback exists), then $Cov(y_2, u_1) \neq 0$. Thus, $y_2$ is endogenous in the first equation, and OLS is inconsistent.
* Direction of Bias (Simultaneity Bias):
o The sign of the asymptotic bias depends on the sign of $\alpha_2 / (1 - \alpha_2 \alpha_1)$.
o Application to Murder Rates (Example 16.1):
* We expect $\alpha_1 < 0$ (police reduce crime) and $\alpha_2 > 0$ (crime increases police hiring).
* If $\alpha_2 > 0$ and $\alpha_2 \alpha_1 < 1$, the bias is positive.
* OLS will overestimate the coefficient $\alpha_1$ (making it less negative or even positive), leading to an underestimate of the effectiveness of police.

Based on the new pictures provided, here is the reproduction of Chapter 16: Simultaneous Equations Models.
16-3 Identifying and Estimating a Structural Equation
16-3a Identification in a Two-Equation System
* Intuition (Supply and Demand):
o Demand equation is identified if there is an observed variable ($z_1$) that shifts supply but does not affect demand directly.
o $z_1$ acts as an instrumental variable for price in the demand equation.
o Supply equation is identified if there is a variable ($z_2$) that shifts demand but not supply.
* General Two-Equation Model:
$$y_1 = \beta_{10} + \alpha_1 y_2 + \mathbf{z}_1 \beta_1 + u_1$$
$$y_2 = \beta_{20} + \alpha_2 y_1 + \mathbf{z}_2 \beta_2 + u_2$$
* Rank Condition for Identification:
o The first equation is identified if and only if the second equation contains at least one exogenous variable (with nonzero coefficient) that is excluded from the first equation.
o Order Condition: Necessary condition. Number of excluded exogenous variables $\ge$ number of included endogenous explanatory variables.
* Example 16.3 (Labor Supply):
o Supply: $hours = \alpha_1 \log(wage) + \dots + u_1$.
o Wage Offer: $\log(wage) = \alpha_2 hours + \dots + u_2$.
o Identification: Supply equation is identified because exper and exper$^2$ appear in the wage offer equation but are excluded from the supply equation (assuming past experience doesn't affect current labor supply preferences).
* Example 16.4 (Inflation and Openness):
o Model:
$$inf = \beta_{10} + \alpha_1 open + \beta_{11} \log(pcinc) + u_1$$
$$open = \beta_{20} + \alpha_2 inf + \beta_{21} \log(pcinc) + \beta_{22} \log(land) + u_2$$
o Eq 16.22 ($inf$) is identified because $\log(land)$ appears in 16.23 but is excluded from 16.22.
o Eq 16.23 ($open$) is not identified because 16.22 contains no exogenous variables excluded from 16.23.
16-3b Estimation by 2SLS
* Once identified, an equation can be estimated by 2SLS.
* Instrumental variables consist of all exogenous variables appearing in the system.
* Example 16.5 (Labor Supply Results):
o Estimated supply curve slopes upward ($\alpha_1 > 0$).
o Elasticity of labor supply $\approx 16.4 (\% \Delta wage)$.
o Comparison: OLS estimate of $\alpha_1$ was negative (-2.05), showing severe simultaneity bias.
* Example 16.6 (Inflation and Openness):
o 2SLS estimate of open on inf is $26.9$, statistically significant.
o Confirms Romer's hypothesis that smaller countries (determined by land) are more open and have lower inflation.
Based on your instructions to avoid duplicating previous sections, I have identified that the new material in this batch begins with Section 16-4 and continues through Section 16-5 and Example 16.7. I have ignored the files corresponding to Chapters 5, 6, and 13 as those were covered in previous turns.
Here is the reproduction of the new notes:
16-4 Systems with More Than Two Equations
* Simultaneous equations models (SEMs) can consist of more than two equations.
* General identification is difficult and requires matrix algebra, but once identified, equations can be estimated by 2SLS.
16-4a Identification in Systems with Three or More Equations
* Example System:
$$y_1 = \alpha_{12} y_2 + \alpha_{13} y_3 + \beta_{11} z_1 + u_1$$
$$y_2 = \alpha_{21} y_1 + \beta_{21} z_1 + \beta_{22} z_2 + \beta_{23} z_3 + u_2$$
$$y_3 = \alpha_{32} y_2 + \beta_{31} z_1 + \beta_{32} z_2 + \beta_{33} z_3 + \beta_{34} z_4 + u_3$$
o $y_g$ are endogenous variables; $z_j$ are exogenous.
* Identification Analysis:
o Equation (16.29) / $y_3$: Contains every exogenous variable in the system. There are no IVs available for $y_2$. Therefore, parameters cannot be consistently estimated (not identified).
o Equation (16.27) / $y_1$: Excludes $z_2, z_3, z_4$. These serve as exclusion restrictions. We have three potential IVs for two endogenous variables ($y_2, y_3$), satisfying the order condition.
Order Condition for Identification
* An equation in any SEM satisfies the order condition if the number of excluded exogenous variables is at least as large as the number of right-hand side endogenous variables.
* Sufficiency Warning: The order condition is necessary but not sufficient. Identification can fail if the excluded exogenous variables do not appear elsewhere in the system (e.g., if $\beta_{34} = 0$, $z_4$ is useless as an IV).
* Terminology:
o Overidentified: More IVs available than needed (e.g., eq 16.27 has 3 excluded vars for 2 endogenous vars).
o Just Identified: Exact match in numbers (e.g., eq 16.28).
o Unidentified: Order condition fails.
16-4b Estimation
* Any identified equation in an SEM can be estimated by 2SLS.
* Instruments: The set of instruments consists of all exogenous variables appearing anywhere in the system.
* System Estimation: Methods like three-stage least squares (3SLS) estimate all equations simultaneously and are generally more efficient, but 2SLS is standard for single equations.
16-5 Simultaneous Equations Models with Time Series
* Applies SEMs to large systems describing a country's economy (e.g., Keynesian aggregate demand models).
Example Model (Consumption, Investment, Income):
$$C_t = \beta_0 + \beta_1 (Y_t - T_t) + \beta_2 r_t + u_{t1}$$
$$I_t = \gamma_0 + \gamma_1 r_t + u_{t2}$$
$$Y_t = C_t + I_t + G_t$$
* Endogenous: $C_t, I_t, Y_t$.
* Exogenous (maintained assumption): $T_t$ (taxes), $r_t$ (interest rate), $G_t$ (government spending).
* Estimation:
o Consumption function (16.30) depends on disposable income (endogenous).
o Instruments for $Y_t$: $T_t, G_t, r_t$.
* Critique: Assumption that taxes, interest rates, and gov spending are exogenous is difficult to justify; governments often adjust these in response to current economic conditions ($Y_t$).
Dynamics and Predetermined Variables
* Adding lags improves realism (e.g., adjustment lags).
$$I_t = \gamma_0 + \gamma_1 r_t + \gamma_2 Y_{t-1} + u_{t2}$$
* Predetermined Variable: A lagged endogenous variable (e.g., $Y_{t-1}$) is treated as exogenous in the current period equation, assuming $u_{t2}$ is uncorrelated with past variables.
* Estimation: Lagged variables are added to the list of instruments (e.g., instruments for consumption function become $T_t, G_t, r_t, C_{t-1}, Y_{t-1}$).
Issues with Time Series SEMs
* Unit Roots: Aggregate consumption and income often have unit roots/trends. Standard OLS/2SLS inference assumes weak dependence; applying them to I(1) variables is complicated.
* Solution: Specify systems in first differences or growth rates.
* Exogeneity: Finding truly exogenous variables is often easier with disaggregated data (e.g., industry level) than aggregate data.
Example 16.7 Testing the Permanent Income Hypothesis
* Model (Campbell and Mankiw 1990):
$$gc_t = \beta_0 + \beta_1 gy_t + \beta_2 r3_t + u_t$$
o $gc_t$: growth in real per capita consumption.
o $gy_t$: growth in real disposable income.
o $r3_t$: real interest rate.
* Hypothesis: Pure PIH implies $\beta_1 = \beta_2 = 0$. Campbell/Mankiw argue $\beta_1 > 0$ if some population consumes current income.
* Endogeneity: $u_t$ contains time $t$ innovations, so it is correlated with $gy_t$ and $r3_t$.
* Instruments: Lagged values $gc_{t-1}, gy_{t-1}, r3_{t-1}$ are valid instruments (uncorrelated with new innovations $u_t$).
* Results:
$$gc_t = 0.0081 + 0.586 gy_t - 0.00027 r3_t$$
o Coefficient on $gy_t$ is 0.586 and significant ($t=4.34$). Pure PIH is strongly rejected.
* Serial Correlation: PIH implies errors are serially uncorrelated. Testing residuals on lagged residuals yields $\hat{\rho} = 0.187$ ($t=1.4$), providing some weak evidence of serial correlation.
Going Further 16.4
* Consider a demand/supply system for fish with monthly data.
* Demand for fish depends on price of fish, income, price of chicken/beef.
* Identification: Assume no seasonality in demand, but seasonality exists in supply.
* Use monthly dummies (seasonality) as instrumental variables for the price of fish to estimate the demand equation.
Here is the reproduction of the notes from the final batch of images (Section 16-4 to Summary), formatted to your specifications.
16-6 Simultaneous Equations Models with Panel Data
* Panel data allows controlling for unobserved heterogeneity ($a_{i1}, a_{i2}$) while dealing with simultaneity.
* Model:
$$y_{it1} = \alpha_1 y_{it2} + \mathbf{z}_{it1} \beta_1 + a_{i1} + u_{it1}$$
* Heterogeneity Endogeneity: Explanatory variables correlated with $a_{i1}$.
* Idiosyncratic Endogeneity: Explanatory variables correlated with $u_{it1}$ (simultaneity).
* Estimation Strategy:
1. First Differencing: Remove $a_{i1}$ by differencing over time:
$$\Delta y_{it1} = \alpha_1 \Delta y_{it2} + \Delta \mathbf{z}_{it1} \beta_1 + \Delta u_{it1}$$
2. Instrumental Variables: $\Delta y_{it2}$ is correlated with $\Delta u_{it1}$. Need instruments for $\Delta y_{it2}$.
3. Instrument Choice: Elements in $\mathbf{z}_{it2}$ (from the second equation) that are excluded from the first equation can serve as instruments.
Example 16.8 Effect of Prison Population on Violent Crime Rates
* Goal: Estimate causal effect of prison population on crime (Levitt 1996).
* Simultaneity: High crime leads to more prisoners; more prisoners reduce crime. OLS is inconsistent.
* Instruments: Instances of prison overcrowding litigation (affect prison growth but not crime directly).
* Model:
$$\log(crime_{it}) = \theta_t + \alpha_1 \log(prison_{it}) + \mathbf{z}_{it1} \beta_1 + a_{i1} + u_{it1}$$
* Estimation: Estimate in first differences (eq 16.41) using pooled 2SLS.
* Results:
o Pooled OLS $\hat{\alpha}_1 = -0.181$.
o Pooled 2SLS $\hat{\alpha}_1 = -1.032$. Effect is much larger when simultaneity is addressed.
* Fixed Effects Alternative:
o Can use Fixed Effects transformation (within transformation) instead of differencing.
o Equation 16.42: Use unit-specific dummy variables and apply 2SLS. Identical to fixed effects 2SLS.
Summary
* SEMs are appropriate when equations have a ceteris paribus causal interpretation (e.g., supply and demand).
* Identification: The first equation is identified if at least one exogenous variable appears in the system but is excluded from that equation (Rank/Order condition).
* Estimation: 2SLS is the standard method for identified equations.
* Distinction: Omitted variables and simultaneity often look similar; 2SLS handles both.
* Extensions:
o Time Series: Must handle trending/integrated processes.
o Panel Data: Can combine First Differencing or Fixed Effects with 2SLS to handle both unobserved heterogeneity and simultaneity.


17 Limited Dependent Variable Models and Sample Selection Corrections
* Overview:
o A Limited Dependent Variable (LDV) is a dependent variable whose range of values is substantively restricted.
o While OLS is often used, it may be inappropriate because linear models do not account for the restricted range (e.g., predicting probabilities outside [0,1] or negative counts).
* Types of LDVs:
1. Binary Response: Takes only two values (zero and one). Example: Employment status.
2. Fractional Response: Variable takes values in the interval [0, 1]. Example: Participation rates in pension plans.
3. Count Variable: Takes nonnegative integer values $\{0, 1, 2, ...\}$. Example: Number of patent applications, number of arrests.
4. Corner Solution Response: Variable is continuous for positive values but has a pileup at zero. Example: Charitable contributions, hours worked.
* Section 17.4 covers the Tobit model for this case.
5. Data Censoring and Sample Selection: Issues where we observe a nonrandom sample. Covered in Sections 17-5 and 17-6.
17-1 Logit and Probit Models for Binary Response
* Interest lies primarily in the response probability:
$$P(y=1|\mathbf{x}) = P(y=1|x_1, x_2, ..., x_k)$$
17-1a Specifying Logit and Probit Models
* Linear Probability Model (LPM) Limitations: Assumes probability is linear in parameters. Fitted probabilities can be $<0$ or $>1$, and partial effects are constant.
* Binary Response Models: To avoid LPM limitations, use a function $G$ strictly between zero and one:
$$P(y=1|\mathbf{x}) = G(\beta_0 + \beta_1 x_1 + ... + \beta_k x_k) = G(\beta_0 + \mathbf{x}\beta)$$
o $G(\cdot)$ is a cumulative distribution function (cdf) of a continuous random variable.
* Logit Model: Uses the standard logistic function:
$$G(z) = \exp(z)/[1 + \exp(z)] = \Lambda(z)$$
* Probit Model: Uses the standard normal cdf:
$$G(z) = \Phi(z) = \int_{-\infty}^z \phi(v)dv$$
o Where $\phi(z) = (2\pi)^{-1/2}\exp(-z^2/2)$ is the standard normal density.
* Latent Variable Model:
o Logit and probit can be derived from an underlying unobserved variable $y^*$:
$$y^* = \beta_0 + \mathbf{x}\beta + e, \quad y = 1[y^* > 0]$$
o $1[\cdot]$ is the indicator function.
o $y=1$ if $y^* > 0$, and $y=0$ if $y^* \le 0$.
o If $e$ is symmetric about zero (like normal or logistic):
$$P(y=1|\mathbf{x}) = P(y^* > 0|\mathbf{x}) = P(e > -(\beta_0 + \mathbf{x}\beta)|\mathbf{x}) = 1 - G(-(\beta_0 + \mathbf{x}\beta)) = G(\beta_0 + \mathbf{x}\beta)$$
* Interpretation of Coefficients:
o The direction of the effect of $x_j$ on $E(y^*|\mathbf{x})$ and $P(y=1|\mathbf{x})$ is determined by the sign of $\beta_j$.
o Magnitudes of $\beta_j$ are not directly interpretable as partial effects on probability (unlike LPM).
* Partial Effects (Continuous Variables):
o Relies on calculus. For a continuous $x_j$:
$$\frac{\partial p(\mathbf{x})}{\partial x_j} = g(\beta_0 + \mathbf{x}\beta)\beta_j, \quad \text{where } g(z) = \frac{dG}{dz}(z)$$
o Since $g(z) > 0$ (pdf), the partial effect has the same sign as $\beta_j$.
o Relative Effects: The ratio of partial effects for two continuous variables equals the ratio of their coefficients, $\beta_j / \beta_h$.
* Discrete Change (Discrete Variables):
o For a discrete variable $x_k$ (e.g., going from $c_k$ to $c_k + 1$):
$$G[\beta_0 + \beta_1 x_1 + ... + \beta_k(c_k + 1)] - G(\beta_0 + \beta_1 x_1 + ... + \beta_k c_k)$$
o Common for binary explanatory variables (e.g., effect of a job training program).
* Functional Forms:
o Can include squares ($x^2$) and logs ($\log x$).
o Elasticity: If $\log(z_2)$ is a regressor, the elasticity of probability with respect to $z_2$ is $\beta_3 [g(\beta_0 + \mathbf{x}\beta) / G(\beta_0 + \mathbf{x}\beta)]$.
17-1b Maximum Likelihood Estimation of Logit and Probit Models
* Nonlinear binary response models are estimated using Maximum Likelihood Estimation (MLE). OLS/WLS are not applicable due to the nonlinear nature of $E(y|\mathbf{x})$.
* Distribution: $y$ given $\mathbf{x}$ follows a Bernoulli distribution.
$$f(y|\mathbf{x};\beta) = [G(\mathbf{x}\beta)]^y [1 - G(\mathbf{x}\beta)]^{1-y}, \quad y=0,1$$
* Log-Likelihood Function:
$$\ell_i(\beta) = y_i \log[G(\mathbf{x}_i\beta)] + (1 - y_i) \log[1 - G(\mathbf{x}_i\beta)]$$
* The MLE $\hat{\beta}$ maximizes the sum of log-likelihoods. Consistency and asymptotic normality follow from general MLE theory.
17-1c Testing Multiple Hypotheses
* Tests:
o Wald Test: Uses unrestricted model. Standard $\chi^2$ distribution.
o Score (LM) Test: Uses restricted model.
o Likelihood Ratio (LR) Test: Based on the difference in log-likelihoods between restricted ($\mathcal{L}_{ur}$) and unrestricted ($\mathcal{L}_{r}$) models.
$$LR = 2(\mathcal{L}_{ur} - \mathcal{L}_{r})$$
o $LR \sim \chi^2_q$ where $q$ is the number of restrictions. $LR$ is always non-negative.
17-1d Interpreting the Logit and Probit Estimates
* Goodness-of-Fit:
o Percent Correctly Predicted: Use a threshold (usually 0.5) to predict $y_i=1$ if fitted probability $> 0.5$. Compare to actual $y_i$.
* Critique: Can be misleading if one outcome is rare (e.g., predicting "no arrest" for everyone might yield 87.5% accuracy but is useless).
o Pseudo R-Squared (McFadden):
$$1 - \mathcal{L}_{ur} / \mathcal{L}_{o}$$
* Where $\mathcal{L}_{o}$ is the log-likelihood of the intercept-only model. Bounded between 0 and 1 (but never reaches 1 for binary data).
* Average Partial Effect (APE):
o To summarize partial effects, we average the individual partial effects across the sample (rather than plugging in the average $\bar{\mathbf{x}}$ into the function, which is the "Partial Effect at the Average" or PEA).
o APE Formula:
$$n^{-1} \sum_{i=1}^n [g(\hat{\beta}_0 + \mathbf{x}_i\hat{\beta}) \hat{\beta}_j]$$
o This is preferred for discrete variables or large changes where calculus approximation fails.
* Comparing Magnitudes (Rule of Thumb):
o To compare slope estimates across models (due to different scale factors):
* $\hat{\beta}_{probit} \approx 0.4$
* $\hat{\beta}_{logit} \approx 0.25$
* Multiply Probit by $1.6$ to compare to Logit.
* Divide Logit by $4$ to compare to LPM (OLS).
* Divide Probit by $2.5$ to compare to LPM (OLS).
Here is the reproduction of the new material found in the latest batch of images (from Example 17.1 through Section 17-1e), avoiding duplication of previous sections.
17-1d Interpreting the Logit and Probit Estimates (Continued)
* Example 17.1 (Married WomenÕs Labor Force Participation):
o Data: 753 married women from MROZ (Example 8.8).
o Comparison of Models: Estimated LPM, Logit, and Probit.
o Consistency: Signs of coefficients and statistical significance of variables are consistent across all three models.
o Goodness of Fit: Pseudo R-squared for Logit/Probit is comparable to the usual R-squared for LPM.
o Scaling Estimates:
* Instead of rule-of-thumb factors (0.4 or 0.25), we can compute specific scale factors at sample averages (PEA).
* For educ:
* LPM Estimate: $0.038$.
* Scaled Logit: $0.179 \times 0.221 \approx 0.040$.
* Scaled Probit: $0.301 \times 0.131 \approx 0.039$.
* All three are remarkably close.
* Table 17.2 Comparison of Average Partial Effects (APEs):
o APEs are very similar across LPM, Logit, and Probit for all explanatory variables.
o Example (kidslt6): Effect of having a small child on probability of working.
* LPM: $-0.262$.
* Logit: $-0.258$.
* Probit: $-0.261$.
o Diminishing Effects: Unlike LPM (constant effect), Logit and Probit imply diminishing magnitudes of partial effects at extreme probabilities.
o Calculus vs. Discrete Change: For kidslt6, evaluating the change in CDF from 0 to 1 (discrete difference) gives a decrease of about $0.334$ (probit) vs $0.256$ (linear), showing that the linear model estimates the effect near the average.
* Visualizing Differences (Figure 17.2):
o Graphs probability of participation against years of education.
o LPM is a straight line; Probit is an S-curve.
o At very low education (5 years) or very high education (17 years), the models diverge slightly, but are similar near the average (12 years).
* Potential Specification Issues:
o Endogeneity: Omitted variables or simultaneity cause bias in Logit/Probit just as in linear models (e.g., endogenous educ). Requires 2-stage methods (Chapter 15).
o Non-Normality: If the latent error $e$ is not normal, Probit is inconsistent. However, if we only care about the direction of effects, this is often a minor issue.
o Heteroskedasticity: If $Var(e|\mathbf{x})$ depends on $\mathbf{x}$, the response probability form $G(\beta_0 + \mathbf{x}\beta)$ is incorrect.
* Unlike in OLS (where heteroskedasticity only affects inference), in Probit/Logit it causes inconsistency of parameter estimates.
* Requires more general estimation if suspected.
17-1e Computing APEs in More Complicated Settings
* Context: Models with interactions or discrete variables where calculus approximations (partial derivatives) are insufficient.
* Example (Job Training):
o Model: $P(y=1|\mathbf{x}, part, full) = G(\beta_0 + \mathbf{x}\beta + \gamma_1 part + \gamma_2 full)$.
o part and full are dummy variables for part-time and full-time participation.
* Average Treatment Effects (ATE):
o We want to estimate the effect of participation levels on the entire population.
o Wrong Approach: Computing partial effects at the average ($\bar{\mathbf{x}}$) makes no sense for binary/discrete variables.
o Correct Approach (ATE): Average the difference in probabilities across all individuals $i$:
$$ATE_{part} = n^{-1} \sum_{i=1}^n [G(\hat{\beta}_0 + \mathbf{x}_i\hat{\beta} + \hat{\gamma}_1) - G(\hat{\beta}_0 + \mathbf{x}_i\hat{\beta})]$$
$$ATE_{full} = n^{-1} \sum_{i=1}^n [G(\hat{\beta}_0 + \mathbf{x}_i\hat{\beta} + \hat{\gamma}_2) - G(\hat{\beta}_0 + \mathbf{x}_i\hat{\beta})]$$
o Crucial Detail: When computing the counterfactuals (e.g., everyone participates vs everyone does not), we must explicitly set the other treatment dummies to zero (e.g., set $full=0$ when calculating $ATE_{part}$) to avoid nonsensical combinations.
* General Policy Analysis:
o With multiple treatment levels ($w_1, w_2, ..., w_J$), the proper ATE relative to a control group is:
$$ATE_j = n^{-1} \sum_{i=1}^n [G(\hat{\beta}_0 + \mathbf{x}_i\hat{\beta} + \hat{\gamma}_j) - G(\hat{\beta}_0 + \mathbf{x}_i\hat{\beta})]$$
o This averages the difference in probabilities for each unit if they were treated ($w_j=1$) versus if they were controls ($all \ w=0$), holding other characteristics $\mathbf{x}_i$ fixed.
Here is the reproduction of the content from the latest batch of images, covering Section 17-2 and Section 17-3.
17-2 Fractional Response Models
* Definition: A fractional response variable takes values in the interval $[0, 1]$, possibly including endpoints (e.g., proportion of employees in a pension plan, fraction of savings in stocks).
* Model Specification:
o Similar to binary response, linear models can predict outside $[0,1]$.
o We model the conditional mean using a function $G(\cdot)$ that enforces the range:
$$E(y|\mathbf{x}) = G(\beta_0 + \mathbf{x}\beta)$$
o Logit ($\Lambda$) and Probit ($\Phi$) functions serve this purpose well.
* Estimation (Bernoulli QMLE):
o We maximize the Bernoulli log-likelihood function (same as in binary response):
$$\ell_i(\beta) = y_i \log[G(\mathbf{x}_i\beta)] + (1 - y_i) \log[1 - G(\mathbf{x}_i\beta)]$$
o Consistency: The estimator is consistent for parameters even though $y$ is not binary (and thus cannot have a Bernoulli distribution), provided the conditional mean is correctly specified (Papke and Wooldridge 1996).
o Inference:
* The distributional assumption (Bernoulli) is incorrect for fractional data.
* Standard errors obtained by treating $y$ as binary are incorrect (usually too small).
* Robust Standard Errors: Must be used to account for the fact that $Var(y|\mathbf{x})$ is not necessarily $G(\cdot)[1-G(\cdot)]$.
* Interpretation:
o Partial effects and APEs are calculated exactly as in the binary case (using calculus or discrete differences).
o Goodness-of-fit can be measured by the squared correlation between $y_i$ and fitted values $\hat{y}_i = G(\hat{\beta}_0 + \mathbf{x}_i\hat{\beta})$.
* Example 17.2 (Participation in 401(k) Pension Plans):
o Outcome: prate (fraction of eligible employees participating).
o Comparison: Linear model vs. Fractional Logit.
o Results:
* Signs of coefficients are comparable.
* APE for match rate (mrate):
* Linear: $0.107$.
* Fractional Logit: $0.147$.
* Fractional logit implies a larger effect of plan generosity.
* Fractional logit predictions are guaranteed to be in $[0,1]$, whereas linear model can predict outside.
o Nonlinearity: Table 17.4 shows that the APE of mrate diminishes as the match rate increases (from $0.195$ at $mrate=0$ to $0.032$ at $mrate=2$), capturing diminishing returns.
17-3 An Exponential Mean Model and Poisson Regression
* Context: Used for nonnegative outcomes $y \ge 0$, especially count variables (e.g., number of patent applications) or corner solution responses with a pileup at zero.
* Limitations of Log-Linear Models:
o Using $\log(y)$ requires $y > 0$. Adjustments like $\log(1+y)$ solve the data problem but have arbitrary properties (e.g., sensitive to units of measurement) and do not solve the "pileup at zero" issue conceptually.
o Interpreting coefficients from $\log(1+y)$ models is difficult.
* Exponential Conditional Mean:
o A better alternative is to model the expected value directly as an exponential function:
$$E(y|\mathbf{x}) = \exp(\beta_0 + \beta_1 x_1 + \dots + \beta_k x_k)$$
o Ensures predicted values are positive.
o Interpretation:
* Log-linear in parameters: $\log[E(y|\mathbf{x})] = \beta_0 + \mathbf{x}\beta$.
* Semi-elasticity: $100\beta_j$ is roughly the percentage change in $E(y|\mathbf{x})$ given a one-unit increase in $x_j$.
* Elasticity: If $x_j = \log(z_j)$, then $\beta_j$ is the elasticity.
* Poisson Regression Model:
o Derived assuming $y$ given $\mathbf{x}$ follows a Poisson distribution:
$$P(y=h|\mathbf{x}) = \exp[-\exp(\mathbf{x}\beta)] [\exp(\mathbf{x}\beta)]^h / h!$$
o Log-Likelihood:
$$\ell_i(\beta) = y_i \mathbf{x}_i\beta - \exp(\mathbf{x}_i\beta)$$
(dropping the $\log(y_i!)$ term).
* Quasi-Maximum Likelihood (Poisson QMLE):
o The Poisson distributional assumption implies $Var(y|\mathbf{x}) = E(y|\mathbf{x})$ (variance equals mean). This is often restrictive (overdispersion is common).
o Robustness Property: The Poisson QMLE (maximizing the Poisson log-likelihood) provides consistent estimators for $\beta$ even if the Poisson distribution is incorrect, as long as the conditional mean $E(y|\mathbf{x}) = \exp(\mathbf{x}\beta)$ is correctly specified.
o Comparison to OLS:
* If we approximate $\exp(\cdot)$ with a linear function, the OLS slope $\hat{\gamma}_j$ is roughly comparable to $\hat{\beta}_j \bar{y}$ (Poisson coeff $\times$ average outcome).
* Inference and Variance Adjustment:
o Since $Var(y|\mathbf{x}) = E(y|\mathbf{x})$ likely fails, standard Poisson errors are invalid.
o Fully Robust: Use sandwich estimator (robust standard errors).
o GLM Approach (Variance Proportional to Mean):
* Assume $Var(y|\mathbf{x}) = \sigma^2 E(y|\mathbf{x})$.
* $\sigma^2$ (dispersion parameter) can be estimated by:
$$\hat{\sigma}^2 = (n - k - 1)^{-1} \sum_{i=1}^n \frac{(y_i - \hat{y}_i)^2}{\hat{y}_i}$$
* Adjustment: Multiply the usual Poisson standard errors by $\hat{\sigma}$.
* Overdispersion: If $\hat{\sigma} > 1$ (common), unadjusted errors are too small.
o Quasi-Likelihood Ratio Statistic: Divide the standard likelihood ratio statistic by $\hat{\sigma}^2$ to test restrictions.

17-4 The Tobit Model for Corner Solution Responses
* Corner solution outcomes occur when a variable is zero for a significant fraction of the population but continuous for positive values (e.g., charitable contributions, hours worked).
* Latent Variable Model:
$$ y^* = \beta_0 + \mathbf{x}\beta + u, \quad u|\mathbf{x} \sim \mathrm{Normal}(0, \sigma^2) \quad [17.28] $$
$$ y = \max(0, y^*) \quad [17.29] $$
.
* The observed variable $ y $ equals $ y^* $ when $ y^* \ge 0 $, and 0 when $ y^* < 0 $.
* Probability of Zero Outcome:
$$ P(y=0|\mathbf{x}) = P(y^* < 0|\mathbf{x}) = 1 - \Phi(\mathbf{x}\beta/\sigma) \quad [17.31] $$
.
* Density of $ y $ given $ \mathbf{x} $:
o For $ y=0 $: Probability is $ 1 - \Phi(\mathbf{x}\beta/\sigma) $.
o For $ y>0 $: Density is $ (1/\sigma)\phi[(y - \mathbf{x}\beta)/\sigma] $.
* Log-Likelihood Function:
$$ \ell_i(\beta, \sigma) = 1(y_i=0)\log[1 - \Phi(\mathbf{x}_i\beta/\sigma)] + 1(y_i>0)\log\{(1/\sigma)\phi[(y_i - \mathbf{x}_i\beta)/\sigma]\} \quad [17.32] $$
.
* MLE is used to estimate $ \beta $ and $ \sigma $.

17-4a Interpreting the Tobit Estimates
* Tobit models are estimated via maximum likelihood; coefficients $ \beta_j $ measure partial effects on the latent variable expectation $ E(y^*|\mathbf{x}) $, not the observed $ y $.
* Two expectations are of interest: Conditional expectation $ E(y|y>0, \mathbf{x}) $ and unconditional expectation $ E(y|\mathbf{x}) $.
* The relationship between expectations is:
$$ E(y|\mathbf{x}) = P(y>0|\mathbf{x}) \cdot E(y|y>0, \mathbf{x}) = \Phi(\mathbf{x}\beta/\sigma) \cdot E(y|y>0, \mathbf{x}) \quad [17.33] $$
.
* The expected value for the subpopulation where $ y $ is positive uses the inverse Mills ratio $ \lambda(c) = \phi(c)/\Phi(c) $:
$$ E(y|y>0, \mathbf{x}) = \mathbf{x}\beta + \sigma\lambda(\mathbf{x}\beta/\sigma) \quad [17.34] $$
.
* The inverse Mills ratio is strictly positive; using OLS only on $ y>0 $ omits this variable, causing inconsistent estimates of $ \beta $.
* Combining [17.33] and [17.34] gives the unconditional expectation, which is nonlinear in $ \mathbf{x} $ and $ \beta $:
$$ E(y|\mathbf{x}) = \Phi(\mathbf{x}\beta/\sigma)[\mathbf{x}\beta + \sigma\lambda(\mathbf{x}\beta/\sigma)] = \Phi(\mathbf{x}\beta/\sigma)\mathbf{x}\beta + \sigma\phi(\mathbf{x}\beta/\sigma) \quad [17.35] $$
.
* Predicted values for $ y $ from [17.35] are always positive.
* The partial effect of $ x_j $ on the conditional expectation $ E(y|y>0, \mathbf{x}) $ is:
$$ \frac{\partial E(y|y>0, \mathbf{x})}{\partial x_j} = \beta_j \{ 1 - \lambda(\mathbf{x}\beta/\sigma)[\mathbf{x}\beta/\sigma + \lambda(\mathbf{x}\beta/\sigma)] \} \quad [17.36] $$
.
* The term in brackets in [17.36] is an adjustment factor strictly between zero and one, dependent on $ \mathbf{x} $ and $ \sigma $.
* $ \sigma $ is not merely an "ancillary" parameter; it directly affects the magnitude of partial effects.
* The elasticity of $ y $ with respect to $ x_1 $ conditional on $ y>0 $ is:
$$ \frac{\partial E(y|y>0, \mathbf{x})}{\partial x_1} \cdot \frac{x_1}{E(y|y>0, \mathbf{x})} \quad [17.37] $$
.
* The partial derivative of the unconditional expectation $ E(y|\mathbf{x}) $ accounts for changes in both the probability of being positive and the expected value given positive:
$$ \frac{\partial E(y|\mathbf{x})}{\partial x_j} = \frac{\partial P(y>0|\mathbf{x})}{\partial x_j} \cdot E(y|y>0, \mathbf{x}) + P(y>0|\mathbf{x}) \cdot \frac{\partial E(y|y>0, \mathbf{x})}{\partial x_j} \quad [17.38] $$
.
* Using the fact that $ \partial P(y>0|\mathbf{x})/\partial x_j = (\beta_j/\sigma)\phi(\mathbf{x}\beta/\sigma) $ [17.39], equation [17.38] simplifies to:
$$ \frac{\partial E(y|\mathbf{x})}{\partial x_j} = \beta_j \Phi(\mathbf{x}\beta/\sigma) \quad [17.40] $$
.
* Equation [17.40] allows comparison with OLS estimates: OLS slope $ \hat{\gamma}_j \approx \hat{\beta}_j \Phi(\mathbf{x}\bar{\beta}/\hat{\sigma}) $.
* Partial Effect at the Average (PEA) evaluates $ \Phi(\mathbf{x}\hat{\beta}/\hat{\sigma}) $ at mean values; Average Partial Effect (APE) averages the individual scale factors $ n^{-1} \sum \Phi(\mathbf{x}_i\hat{\beta}/\hat{\sigma}) $.
* APE is preferred to PEA; the scale factor is always between 0 and 1, so Tobit estimates $ \hat{\beta}_j $ are generally larger than OLS.
* For discrete variables, estimate the partial effect as $ E(y|\mathbf{x}) $ with $ x_1=1 $ minus $ E(y|\mathbf{x}) $ with $ x_1=0 $.
Example 17.4 Married WomenÕs Annual Labor Supply
* Data: 753 married women from MROZ file; 428 worked positive hours.
* Table 17.6: Tobit coefficient estimates have the same sign as OLS but are larger in magnitude (e.g., educ is 80.65 for Tobit vs 28.76 for OLS).
* To compare magnitudes, multiply Tobit estimates by the APE scale factor (approx 0.589); adjusted educ effect is $ \approx 47.5 $, still larger than OLS (28.8).
* Table 17.7: Reports APEs. Tobit APEs for nwifeinc, educ, and kidslt6 are larger in magnitude than OLS coefficients; exper and age are similar.
* Goodness of fit: Standard $ R^2 $ is inappropriate. Using the squared correlation between $ y_i $ and $ \hat{y}_i $ (estimated $ E(y|\mathbf{x}) $), Tobit fits slightly better (0.274) than OLS (0.266).
* Figure 17.3: Plots expected hours vs education. OLS is linear and predicts negative hours for ~5% of sample. Tobit is nonlinear (convex), predicts only positive hours, and shows increasing marginal returns to education.
Would you like me to process the next section on "Specification Issues in Tobit Models"?
17-4b Specification Issues in Tobit Models
* Tobit formulas for expectations rely crucially on normality and homoskedasticity in the latent variable model.
* Unlike OLS, where heteroskedasticity only affects efficiency and standard errors, in Tobit models, if assumptions fail, the MLE is hard to interpret.
* A key limitation is that the effect of $ x_j $ on the probability $ P(y>0|\mathbf{x}) $ is proportional to its effect on the conditional mean $ E(y|y>0, \mathbf{x}) $; both depend on $ \beta_j $ and $ \mathbf{x}\beta/\sigma $.
* This proportionality rules out cases where a variable affects the probability of participation differently than the amount of participation (e.g., life insurance and age).
* Diagnostic check: Estimate a probit model for $ y>0 $ to get $ \hat{\gamma}_j $. If Tobit holds, $ \hat{\gamma}_j $ should be close to $ \hat{\beta}_j / \hat{\sigma} $.
* Significant differences in sign or magnitude between $ \hat{\gamma}_j $ and $ \hat{\beta}_j / \hat{\sigma} $ suggest Tobit is inappropriate.
* Alternative models: Hurdle or two-part models allow $ P(y>0|\mathbf{x}) $ and $ E(y|y>0, \mathbf{x}) $ to depend on different parameters.
17-5 Censored and Truncated Regression Models
* It is crucial to distinguish between corner solution outcomes (Tobit), data censoring, and data truncation.
* Censored regression: We solve a data collection problem where outcome data is missing for a subset, but $ x $ is known for all units.
* Truncated regression: We exclude a subset of the population from the sample based on the value of $ y $; both $ y $ and $ x $ are unobserved for this subset.
17-5a Censored Regression Models
* The censored normal regression model assumes a classical linear model for the variable $ y_i $:
$$ y_i = \beta_0 + \mathbf{x}_i \beta + u_i, \quad u_i | \mathbf{x}_i, c_i \sim \mathrm{Normal}(0, \sigma^2) \quad [17.41] $$
.
* We observe $ w_i $ instead of $ y_i $, based on a censoring value $ c_i $:
$$ w_i = \min(y_i, c_i) \quad [17.42] $$
.
* This formulation represents right censoring (e.g., top coding in wealth surveys where values above a threshold are reported as "at least X").
* OLS regression using only uncensored observations produces inconsistent estimators.
* Estimation is performed via Maximum Likelihood (MLE). The density differs for censored and uncensored observations:
o For uncensored ($ w_i = y_i < c_i $): Density is normal pdf.
o For censored ($ w_i = c_i $): Probability is $ P(w_i = c_i|\mathbf{x}_i) = P(u_i \ge c_i - \mathbf{x}_i\beta) = 1 - \Phi([c_i - \mathbf{x}_i\beta]/\sigma) $.
* The density of $ w_i $ given $ \mathbf{x}_i $ and $ c_i $ combines these parts:
$$ f(w|\mathbf{x}_i, c_i) = 1 - \Phi([c_i - \mathbf{x}_i\beta]/\sigma), \quad w = c_i \quad [17.43] $$
$$ = (1/\sigma)\phi([w - \mathbf{x}_i\beta]/\sigma), \quad w < c_i \quad [17.44] $$
.
* Duration analysis is a common application, often using log(duration) as the dependent variable to satisfy normality and handling censoring (e.g., unemployment spells that last longer than the observation period).
Example 17.5 Duration of Recidivism
* Data: RECID file with 1,445 inmates; 893 had not been arrested during the follow-up (censored observations).
* Model: Censored normal regression for $ \log(durat) $.
* Interpretation: Coefficients multiplied by 100 give the estimated percentage change in expected duration.
* Findings:
o priors (prior convictions) and tserved (time served) have negative effects on time until next arrest (proclivity for crime).
o A man serving time for a felony has an expected duration approx 56% longer ($ \exp(0.444)-1 \approx 0.56 $).
o workprg (work program participation) coefficient is small and insignificant, suggesting no effect, though self-selection bias is possible.
* Comparison with OLS: OLS on the entire sample (treating censored values as actual) yields markedly different estimates (shrunk toward zero), confirming censored regression is more reliable.
17-5b Truncated Regression Models
* Truncated regression applies when units are randomly sampled from a population but we only observe units where the outcome $ y $ falls within a certain range (e.g., $ y \le c $).
* Unlike censoring, if a unit is truncated, we have no information on explanatory variables $ \mathbf{x} $ either.
* Example: Hausman and Wise (1977) income maintenance experiment analysis using only families with income below 1.5 times the poverty line.
* Truncated Normal Regression Model:
$$ y = \beta_0 + \mathbf{x}\beta + u, u|\mathbf{x} \sim \mathrm{Normal}(0, \sigma^2) \quad [17.45] $$
.
* We observe $ (\mathbf{x}_i, y_i) $ only if $ y_i \le c_i $.
* The density of $ y_i $ given $ \mathbf{x}_i $ and the truncation rule is re-normalized by dividing by the probability of selection:
$$ g(y|\mathbf{x}_i, c_i) = \frac{f(y|\mathbf{x}_i, \beta, \sigma^2)}{F(c_i|\mathbf{x}_i, \beta, \sigma^2)}, \quad y \le c_i \quad [17.46] $$
.
* $ f(\cdot) $ is the normal density and $ F(\cdot) $ is the normal CDF.
* Bias of OLS: Applying OLS to a truncated sample produces estimates biased toward zero (slope is flattened). This is because truncating the dependent variable reduces the variation in $ y $ correlated with $ x $.
* Figure 17.4: Shows how the regression line for the truncated population (observed incomes < $50,000) is flatter than the true population regression line.

17-6 Sample Selection Corrections
* Nonrandom sample selection arises not just from survey design but when respondents fail to provide answers, leading to missing data.
* Incidental truncation occurs when we do not observe $ y $ because of the outcome of another variable (e.g., we only observe a wage offer if a person is in the workforce).
* Incidental truncation often requires more sophisticated corrections than simple missing data problems.
17-6a When Is OLS on the Selected Sample Consistent?
* It is crucial to distinguish between exogenous and endogenous sample selection.
* Start with a population model for a random draw:
$$ y_i = \mathbf{x}_i \beta + u_i, \quad E(u|x_1, \dots, x_k) = 0 \quad [17.47, 17.48] $$
.
* Define a selection indicator $ s_i $, where $ s_i = 1 $ if the observation is used, and $ s_i = 0 $ otherwise.
* We estimate the equation using only the selected sample ($ s_i = 1 $):
$$ s_i y_i = s_i \mathbf{x}_i \beta + s_i u_i \quad [17.49] $$
.
* Consistency:
o OLS is consistent if the error term $ s_i u_i $ has zero mean and is uncorrelated with $ s_i \mathbf{x}_i $.
o If selection $ s $ is independent of $ (\mathbf{x}, u) $, OLS is consistent and unbiased.
o If selection $ s $ depends on $ \mathbf{x} $ but is independent of $ u $ (exogenous sample selection), OLS is still consistent and unbiased.
o Endogenous sample selection: If $ s $ depends on $ u $ (directly or via unobservables), OLS is inconsistent (e.g., truncated samples where $ s=1 $ if $ y \le c $).
17-6b Incidental Truncation
* This form of selection happens when we observe $ y $ only for a subset of the population, and the rule determining selection depends on another variable.
* Example: Wage offer $ wage^\circ $ is observed only if the individual works ($ wage^\circ \ge reservation \ wage $).
* Structural Model:
$$ y = \mathbf{x}\beta + u, \quad E(u|\mathbf{x}) = 0 \quad [17.51] $$
.
* Selection Equation:
$$ s = 1[\mathbf{z}\gamma + v \ge 0] \quad [17.52] $$
where $ s=1 $ if we observe $ y $, and $ \mathbf{z} $ is a set of exogenous variables (some of which may be in $ \mathbf{x} $).
* Assumptions: $ (u, v) $ are bivariate normal with correlation $ \rho $, standard deviation of $ v $ normalized to 1, and independent of $ \mathbf{z} $.
* Conditional Expectation: The expected value of $ y $ given selection is:
$$ E(y|\mathbf{z}, s=1) = \mathbf{x}\beta + E(u|\mathbf{z}, v \ge -\mathbf{z}\gamma) = \mathbf{x}\beta + \rho E(v|v \ge -\mathbf{z}\gamma) $$
$$ E(y|\mathbf{z}, s=1) = \mathbf{x}\beta + \rho \lambda(\mathbf{z}\gamma) \quad [17.53] $$
where $ \lambda(\cdot) $ is the inverse Mills ratio.
* Equation [17.53] shows that running OLS of $ y $ on $ \mathbf{x} $ on the selected sample suffers from omitted variable bias if $ \rho \ne 0 $, as $ \lambda(\mathbf{z}\gamma) $ is omitted and generally correlated with $ \mathbf{x} $.
Sample Selection Correction (Heckit Method)
* The Heckit method (Heckman, 1976) is a two-step procedure to consistently estimate $ \beta $:
1. Probit Estimation: Estimate the selection equation (probit of $ s_i $ on $ \mathbf{z}_i $) using all $ n $ observations to obtain $ \hat{\gamma} $. Compute the inverse Mills ratio $ \hat{\lambda}_i = \lambda(\mathbf{z}_i \hat{\gamma}) $ for each observation where $ s_i = 1 $.
2. Regression: Regress $ y_i $ on $ \mathbf{x}_i $ and $ \hat{\lambda}_i $ using only the selected sample:
$$ y_i \text{ on } \mathbf{x}_i, \hat{\lambda}_i \quad [17.55] $$
.
* Testing for Selection Bias: The usual $ t $ statistic on $ \hat{\lambda}_i $ is a valid test for $ H_0: \rho = 0 $ (no sample selection bias).
* Exclusion Restriction: To avoid high multicollinearity between $ \hat{\lambda}_i $ and $ \mathbf{x}_i $, $ \mathbf{z} $ should ideally contain at least one element that is not in $ \mathbf{x} $ (a variable that affects selection but has no partial effect on $ y $).
* If $ \mathbf{z} = \mathbf{x} $, identification relies solely on the nonlinearity of the inverse Mills ratio, which can result in very high standard errors.
Example 17.6 Wage Offer Equation for Married Women
* Objective: Estimate the wage offer equation for married women using the MROZ data, correcting for the fact that wages are only observed for working women (incidental truncation).
* Model:
o Selection (Probit): Labor force participation depends on educ, exper, age, nwifeinc, kidslt6, kidsge6.
o Structural (Wage): Log(wage) depends on educ, exper, age.
o Exclusion Restriction: nwifeinc, kidslt6, and kidsge6 are included in the selection equation but excluded from the wage equation (assumed to affect the decision to work but not productivity).
* Results (Table 17.9):
o The estimated coefficient on the inverse Mills ratio $ \hat{\lambda} $ is 0.032 with a $ t $ statistic of 0.239.
o Conclusion: We fail to reject the null hypothesis of no sample selection bias. OLS estimates are very close to Heckit estimates (e.g., return to education is approx 10.8% for OLS and 10.9% for Heckit).
o Standard errors for Heckit are slightly larger, which is expected.
Endogenous Explanatory Variables and Sample Selection
* It is possible to have models with both endogenous explanatory variables and sample selection.
* Model:
$$ y_1 = \alpha_1 y_2 + \mathbf{z}_1 \beta_1 + u_1 \quad [17.56] $$
observed only when $ s=1 $.
* Estimation requires two exogenous variables outside the structural equation: one to instrument for $ y_2 $ (endogeneity) and one to affect selection $ s $ (selection bias).
Summary
* Logit and Probit: Used for binary response variables. Fitted probabilities are bounded between 0 and 1, unlike the LPM. Interpretation is harder (nonlinear parameters).
* Count Data (Poisson/Exponential): $ E(y|\mathbf{x}) = \exp(\beta_0 + \mathbf{x}\beta) $. Ensures positive predictions and parameters act as semi-elasticities.
* Tobit Model: Applicable to corner solution outcomes (nonnegative variables piling up at zero). Estimates partial effects on both the latent and observed variable.
* Censored Regression: Used when data on the dependent variable is missing (e.g., top coding) but explanatory variables are observed. MLE is consistent, while OLS is not.
* Truncated Regression: Used when a part of the population is entirely excluded (both $ y $ and $ x $ unobserved) based on the value of $ y $.
* Sample Selection Correction: HeckmanÕs method corrects for incidental truncation (data missing due to another variable) by including the inverse Mills ratio as an additional regressor.

Wooldrigde (2010):
Chapter 5 (manger)
Here is the reproduction of the provided notes, formatted exactly as requested for your Word document.
5 Instrumental Variables Estimation of Single-Equation Linear Models
* IV estimation is second only to OLS in empirical economic research.
* Addresses models where unobservable error is correlated with explanatory variables.
5.1 Instrumental Variables and Two-Stage Least Squares
5.1.1 Motivation for Instrumental Variables Estimation
The Population Model
* Consider the linear population model:
$$y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_K x_K + u$$
* Assumptions for exogenous variables:
$$E(u) = 0, \quad Cov(x_j, u) = 0, \quad j=1, 2, \dots, K-1$$
* The Problem: $x_K$ might be correlated with $u$ (endogenous), while $x_1, \dots, x_{K-1}$ are exogenous.
* Endogeneity sources: Omitted variables, measurement error, simultaneity.
* OLS estimation results in inconsistent estimators of all $\beta_j$ if $Cov(x_K, u) \neq 0$.
IV Requirements
* To solve endogeneity, we need an observable instrumental variable, $z_1$, that satisfies two conditions:
1. Exogeneity (Condition 5.3): $z_1$ must be uncorrelated with $u$.
$$Cov(z_1, u) = 0$$
2. Relevance (Condition 5.5): $z_1$ must be correlated with $x_K$ after netting out other exogenous variables.
* Defined via the linear projection (reduced form) of $x_K$ onto all exogenous variables:
$$x_K = \delta_0 + \delta_1 x_1 + \delta_2 x_2 + \dots + \delta_{K-1} x_{K-1} + \theta_1 z_1 + r_K$$
* where $E(r_K)=0$ and $r_K$ is uncorrelated with all RHS variables.
* The key assumption is that the coefficient on $z_1$ is nonzero:
$$\theta_1 \neq 0$$
Interpretation of Conditions
* $\theta_1 \neq 0$ implies $z_1$ is partially correlated with $x_K$.
* If $x_K$ is the only explanatory variable, condition (5.5) simplifies to $Cov(z_1, x_K) \neq 0$.
* No restrictions are placed on the distribution of $x_K$ or $z_1$ (can be binary, discrete, or continuous).
* The full list of instrumental variables includes the exogenous variables ($x_1, \dots, x_{K-1}$) acting as their own instruments, plus $z_1$.
The Reduced Form Equation for y
* Substituting the reduced form for $x_K$ (eq 5.4) into the structural equation (eq 5.1) yields the reduced form for $y$:
$$y = \alpha_0 + \alpha_1 x_1 + \dots + \alpha_{K-1} x_{K-1} + \lambda_1 z_1 + v$$
* Where the reduced-form error is $v = u + \beta_K r_K$.
* $v$ is uncorrelated with all explanatory variables in the reduced form.
* OLS consistently estimates the reduced-form parameters $\alpha_j$ and $\lambda_1$.
Identification and Estimation
* Identification Strategy: Using matrix notation where $\mathbf{z} = (1, x_2, \dots, x_{K-1}, z_1)$:
o Assumptions imply population orthogonality:
$$E(\mathbf{z}'u) = \mathbf{0}$$
o Multiplying structural equation by $\mathbf{z}'$ and taking expectations:
$$E(\mathbf{z}'\mathbf{x})\beta = E(\mathbf{z}'y)$$
o This system has a unique solution if the rank condition holds (ruled out perfect collinearity and $\theta_1 \neq 0$):
$$rank \ E(\mathbf{z}'\mathbf{x}) = K$$
* The Estimator: Given a random sample, the IV estimator of $\beta$ is:
$$\hat{\beta} = (N^{-1} \sum_{i=1}^N \mathbf{z}_i' \mathbf{x}_i)^{-1} (N^{-1} \sum_{i=1}^N \mathbf{z}_i' y_i) = (\mathbf{Z}'\mathbf{X})^{-1} \mathbf{Z}'\mathbf{Y}$$
* Consistency follows from the law of large numbers.
Testing the IV Conditions
* Relevance (5.5): Can be tested. Run OLS on the reduced form equation for $x_K$ and use a t-test (often heteroskedasticity-robust) for $H_0: \theta_1 = 0$.
* Exogeneity (5.3): Cannot be tested because it involves the unobservable $u$. Must rely on economic intuition/theory.
Instrument Validity vs. Proxy Variables
* Contrast with Proxy Variables: A proxy must be highly correlated with the omitted variable.
* An IV must be uncorrelated with the omitted variable (and the error term).
* A proxy makes a poor IV, and an IV makes a poor proxy.
Examples and Applications
* Example 5.1 (Wage Equation):
o Model: 
$$\log(wage) = \beta_0 + \beta_1 exper + \beta_2 exper^2 + \beta_3 educ + u$$
o Endogeneity: educ correlated with omitted ability/background in $u$.
o Proposed IV: Mother's education (motheduc).
o Validity: Must be uncorrelated with $u$ (ability) but correlated with educ.
o Risk: motheduc might be correlated with unobserved ability/background (invalidating exogeneity).
* Weak/Poor Instruments:
o SSN Last Digit: Random (satisfies exogeneity) but uncorrelated with education (fails relevance).
o Quarter of Birth (Angrist and Krueger, 1991): Used compulsory schooling laws to argue people born earlier in the year have less schooling. Validity disputed (bound, Jaeger, Baker 1995) due to weak partial correlation leading to finite sample problems.
* Natural Experiments:
o Occurs when setup produces exogenous variation in an otherwise endogenous variable.
o Vietnam Draft (Angrist, 1990): Draft lottery number (random) used as IV for veteran status to estimate effect on earnings.
o Geography (Hoxby, 2000): Streams/Rivers (natural boundaries) used as IV for school district concentration to estimate effect of school competition.
o Elections (Levitt, 1997): Election timing used as IV for police force size to estimate effect on crime.
o Catholic Schools (Evans and Schwab, 1995): Catholic religious affiliation used as IV for attending Catholic high school.
* Example 5.2 (College Proximity as an IV for Education):
o Card (1995) uses "grew up near a 4-year college" as IV for schooling.
o IV Estimate of return to schooling: 13.2%.
o OLS Estimate: 7.5%.
o Interpretation: OLS may suffer from attenuation bias (measurement error), or IV is not entirely exogenous (location choice endogenous).
5.1.2 Multiple Instruments: Two-Stage Least Squares
* Consider the model where $x_K$ is endogenous, but we have multiple instruments $z_1, z_2, ..., z_M$ such that:
$$Cov(z_h, u) = 0, \quad h=1, 2, ..., M$$
* Intuition: 2SLS chooses the linear combination of all $z$ variables that is most highly correlated with $x_K$.
* First Stage: Define the reduced form for $x_K$:
$$x_K = \delta_0 + \delta_1 x_1 + ... + \delta_{K-1} x_{K-1} + \theta_1 z_1 + ... + \theta_M z_M + r_K$$
o The fitted values $\hat{x}_K$ represent the part of $x_K$ uncorrelated with $u$:
$$\hat{x}_{iK} = \hat{\delta}_0 + \hat{\delta}_1 x_{i1} + ... + \hat{\theta}_M z_{iM}$$
* The Estimator: Using fitted values as instruments for $x_K$ gives the 2SLS estimator:
$$\hat{\beta} = (\sum_{i=1}^N \hat{\mathbf{x}}_i' \hat{\mathbf{x}}_i)^{-1} (\sum_{i=1}^N \hat{\mathbf{x}}_i' y_i)$$
o This is called "two-stage least squares" because it can be obtained by:
1. Regress $x_K$ on all exogenous variables ($z$) to get $\hat{x}_K$.
2. Run OLS of $y$ on exogenous $x$'s and $\hat{x}_K$.
* Important Notes on Procedure:
o Do not perform the two-step procedure manually; standard errors in the second stage will be incorrect.
o Omitting exogenous variables ($x_1...x_{K-1}$) from the first stage produces inconsistent estimators.
o If $M=1$ (one instrument), 2SLS is identical to the standard IV estimator.
* Rank Condition with Multiple Instruments:
o We need at least one exogenous variable not in the structural equation to induce variation in $x_K$.
o Test the null hypothesis in the reduced form (5.14):
$$H_0: \theta_1 = 0, \theta_2 = 0, ..., \theta_M = 0$$
o If we cannot reject $H_0$ at a small significance level, the instruments are weak.
o Overidentification: When $M > 1$, the model is overidentified (more instruments than needed).
5.2 General Treatment of Two-Stage Least Squares
5.2.1 Consistency
* ASSUMPTION 2SLS.1: For some $1 \times L$ vector $\mathbf{z}$,
$$E(z'u) = 0$$
o Unless every element of $x$ is exogenous, $z$ must contain variables from outside the model.
* ASSUMPTION 2SLS.2:
o (a) $rank \ E(z'z) = L$
o (b) $rank \ E(z'x) = K$
o Part (b) is the crucial rank condition for identification, meaning $z$ is sufficiently linearly related to $x$.
* Order Condition:
o Necessary condition: $L \geq K$ (at least as many instruments as explanatory variables).
* Identification Logic:
o Using linear projection $x^* = z\Pi$, we can write $x = x^* + r$.
o $\beta$ is identified by $\beta = [E(x^{*'}x)]^{-1} E(x^{*'}y)$ provided $E(x^{*'}x)$ is nonsingular.
o $E(x^{*'}x)$ is nonsingular if and only if $E(z'x)$ has rank $K$.
* The 2SLS Estimator Formula:
$$\hat{\beta} = \left[ \left( \sum_{i=1}^N \mathbf{x}_i' \mathbf{z}_i \right) \left( \sum_{i=1}^N \mathbf{z}_i' \mathbf{z}_i \right)^{-1} \left( \sum_{i=1}^N \mathbf{z}_i' \mathbf{x}_i \right) \right]^{-1} \left( \sum_{i=1}^N \mathbf{x}_i' \mathbf{z}_i \right) \left( \sum_{i=1}^N \mathbf{z}_i' \mathbf{z}_i \right)^{-1} \left( \sum_{i=1}^N \mathbf{z}_i' y_i \right)$$
* theorem 5.1 (Consistency of 2SLS):
o Under Assumptions 2SLS.1 and 2SLS.2, the 2SLS estimator obtained from a random sample is consistent for $\beta$.
o Proof: Write the estimator in terms of averages and apply the law of large numbers and Slutsky's theorem to each term.
5.2.2 Asymptotic Normality of Two-Stage Least Squares
* ASSUMPTION 2SLS.3: $E(u^2z'z) = \sigma^2E(z'z)$, where $\sigma^2 = E(u^2)$. This is the homoskedasticity assumption.
* theorem 5.2 (Asymptotic Normality): Under Assumptions 2SLS.1Ð2SLS.3, $\sqrt{N}(\hat{\beta} - \beta)$ is asymptotically normally distributed with variance matrix:
$$\sigma^2([E(x'z)][E(z'z)]^{-1}E(z'x))^{-1}$$
* Estimating Variance: $\sigma^2$ is estimated using 2SLS residuals $\hat{u}_i = y_i - \mathbf{x}_i \hat{\beta}$:
$$\hat{\sigma}^2 = (N-K)^{-1} \sum_{i=1}^N \hat{u}_i^2$$
* Important: Residuals must be calculated using original $x$ values, not fitted values from the first stage.
5.2.3 Asymptotic Efficiency of Two-Stage Least Squares
* theorem 5.3 (Relative Efficiency): Under Assumptions 2SLS.1Ð2SLS.3, the 2SLS estimator is efficient in the class of all IV estimators using instruments linear in $z$.
* Implication: Asymptotically, we always do better (or at least no worse) by using as many available instruments as possible, provided they are exogenous.
* If assumption 2SLS.3 (homoskedasticity) fails, a more efficient estimator (GMM) exists.
5.2.4 Hypothesis Testing with Two-Stage Least Squares
* Single Parameter: Testing hypotheses about a single $\beta_j$ uses an asymptotic $t$ statistic.
* Multiple Restrictions: To test $H_0: \mathbf{R}\beta = \mathbf{r}$, we can use a Wald statistic or an $F$-type statistic.
* F-statistic Formula:
$$F = \frac{(SSR_r - SSR_{ur})/q}{SSR_{ur}/(N-K)}$$
o Where $SSR_r$ and $SSR_{ur}$ are sum of squared residuals from the restricted and unrestricted 2SLS regressions.
* Crucial Note: $SSR$ must be computed using the proper 2SLS residuals. Using the residuals from the manual second-stage OLS regression yields an invalid statistic that can even be negative.
* LM Statistic: An alternative based on residuals ($\tilde{u}_i$) from the restricted model:
$$LM = N R_u^2$$
o Where $R_u^2$ is from regressing $\tilde{u}_i$ on all exogenous variables.
Here is the compact reproduction of the notes from the provided images, formatted according to your requirements.
5.2.5 Heteroskedasticity-Robust Inference for Two-Stage Least Squares
* When 2SLS.3 fails, use a robust variance matrix estimator:
$$(\hat{X}'\hat{X})^{-1} (\sum \hat{u}_i^2 \hat{x}_i' \hat{x}_i) (\hat{X}'\hat{X})^{-1}$$
o (Formula approximated from description and standard robust form context).
5.2.6 Potential Pitfalls with Two-Stage Least Squares
* Finite Sample Bias: IV estimators are never unbiased; moments may not exist (Kinal 1980).
* Weak Instruments Inconsistency:
o IV Inconsistency:
$$plim \ \hat{\beta}_{IV} = \beta_1 + \frac{Corr(z_1, u)}{Corr(z_1, x_1)} \frac{\sigma_u}{\sigma_x}$$
o If correlation between $z$ and $x$ is small, a tiny correlation between $z$ and $u$ causes massive inconsistency, potentially worse than OLS.
* Variance Inflation: Weak instruments lead to small explained variation in $\hat{x}_k$, inflating variance.
* Shea's Measure: Partial $R^2$ measure for instrument relevance in multivariate settings.
* Staiger and Stock (1997): If instruments are weak (coefficients decay at $1/\sqrt{N}$), 2SLS is inconsistent and non-normal even in large samples; F-statistic guidelines suggested.
I understand completely. Here is the reproduction of only the information found in the last batch of pictures (Section 5.3 onwards), avoiding any duplication of previous sections.
5.3 IV Solutions to the Omitted Variables and Measurement Error Problems
* Surveys approaches using IV methods to solve omitted variable problems, including measurement error.
5.3.1 Leaving the Omitted Factors in the Error Term
* Model: Consider the omitted variable model:
$$y = \beta_0 + \beta_1 x_1 + ... + \beta_K x_K + \gamma q + v$$
* Solution Strategy: Put the omitted variable $q$ into the error term and find instruments for any element of $\mathbf{x}$ correlated with $q$.
* Instrument Requirements:
1. Redundant in the structural model $E(y|\mathbf{x}, q)$.
2. Uncorrelated with the omitted variable, $q$.
3. Sufficiently correlated with the endogenous elements of $\mathbf{x}$ (those correlated with $q$).
5.3.2 Solutions Using Indicators of the Unobservables
* Overview: Similar to the OLS proxy variable solution but requires IV estimation. Assume we have an indicator $q_1$ of $q$.
* Indicator Equation:
$$q_1 = \delta_0 + \delta_1 q + a_1$$
* Assumptions:
$$Cov(q, a_1) = 0, \quad Cov(\mathbf{x}, a_1) = 0$$
o This includes the classical errors-in-variables model where $\delta_0 = 0$ and $\delta_1 = 1$.
* Contrast with Proxy Variable:
o Rearranging the indicator equation:
$$q = -(\delta_0/\delta_1) + (1/\delta_1)q_1 - (1/\delta_1)a_1$$
o The error term $-(1/\delta_1)a_1$ is necessarily correlated with $q_1$, making the OLS-proxy solution inconsistent.
* Multiple Indicators:
o Requires a second indicator $q_2$:
$$q_2 = \rho_0 + \rho_1 q + a_2$$
o Crucial Assumption: $a_1$ and $a_2$ are uncorrelated:
$$Cov(a_1, a_2) = 0$$
o This implies correlation between $q_1$ and $q_2$ arises only through common dependence on $q$.
* Estimation Strategy:
o Plug $q_1$ in for $q$ in the structural equation:
$$y = \alpha_0 + \mathbf{x}\beta + \gamma_1 q_1 + (v - \gamma_1 a_1)$$
o $q_2$ is uncorrelated with the composite error because it is redundant (uncorrelated with $v$) and uncorrelated with $a_1$.
o Since $q_1$ and $q_2$ are correlated, $q_2$ serves as an IV for $q_1$ (this is the "multiple indicator solution").
o Note: This differs from leaving $q$ in the error term because we do not need to know which elements of $\mathbf{x}$ are correlated with $q$; elements of $\mathbf{x}$ serve as their own instruments.
* Example 5.5 (IQ and KWW as Indicators of Ability):
o Uses $KWW$ (knowledge of working world) as an instrument for $IQ$ in a wage equation.
o Result: Return to education is small (2.5%) and statistically insignificant when using this IV method.
* Classical Errors-in-Variables (CEV) Model:
o Special case where $q_1$ and $q_2$ are measures of $q$ with uncorrelated measurement errors ($\delta_0 = p_0 = 0$ and $\delta_1 = p_1 = 1$).
o Plugging one measure into the equation and using the other as its instrument provides consistent estimators.
* Single Indicator Case:
o If only one indicator $q_1$ is available, parameters are generally not identified.
o Solution: Requires additional variables that are redundant in structural equation, uncorrelated with indicator error $a_1$, but correlated with $q$ to act as instruments for $q_1$.

5.3 IV Solutions to the Omitted Variables and Measurement Error Problems (Continued)
* Classical Errors-in-Variables (CEV):
o In the CEV model, we assume two measures $q_1$ and $q_2$ of $q$ have uncorrelated measurement errors ($\delta_0 = p_0 = 0, \delta_1 = p_1 = 1$).
o Solution: Plugging one measure into the equation and using the other as its instrument provides consistent estimators.
* Single Indicator Solution:
o If only one indicator $q_1$ is available, parameters are generally not identified without further information.
o Strategy: Utilize additional variables that are:
1. Redundant in the structural equation (uncorrelated with $v$).
2. Uncorrelated with the indicator error $a_1$.
3. Correlated with $q$.
o These variables can serve as instruments for $q_1$ to produce consistent estimators (Blackburn and Neumark, 1992).

6 Additional Single-Equation Topics
6.1 Estimation with Generated Regressors and Instruments
* Discusses properties of OLS and 2SLS when regressors or instruments are estimated in a first step.
6.1.1 Ordinary Least Squares with Generated Regressors
* The Model:
$$y = \beta_0 + \beta_1 x_1 + ... + \beta_K x_K + \gamma q + u$$
* The Problem: $q$ is unobserved but related to observable data $w$ by $q = f(w, \delta)$.
* Procedure:
1. Estimate $\delta$ to get $\hat{\delta}$.
2. Obtain generated regressor $\hat{q}_i = f(w_i, \hat{\delta})$.
3. Replace $q_i$ with $\hat{q}_i$ in the OLS regression.
* Consistency: Replacing $q$ with $\hat{q}$ generally produces consistent estimates if $\hat{\delta}$ is consistent.
* Inference (Standard Errors):
o Standard errors and test statistics from the second step are generally invalid because they ignore sampling variation in $\hat{\delta}$.
o Exception: If $H_0: \gamma = 0$ is true, the sampling variation of $\hat{\delta}$ can be ignored asymptotically.
* Condition: $E[\nabla_\delta f(w, \delta)'u] = 0$ (usually implied by zero conditional mean).
o Implication: Can use usual $t$ statistic to test if $q$ belongs in the model ($H_0: \gamma=0$). If $\gamma \neq 0$, variance matrix must be adjusted to account for estimation of $\delta$.
6.1.2 Two-Stage Least Squares with Generated Instruments
* Scenario: Instruments $z$ are estimated in a preliminary stage, $z = g(w, \lambda)$. Generated instruments are $\hat{z}_i = g(w_i, \hat{\lambda})$.
* Inference:
o If condition (6.8) holds ($E[\nabla_\lambda g(w, \lambda)'u] = 0$), the asymptotic distribution of $\hat{\beta}$ is the same whether we use $\lambda$ or $\hat{\lambda}$.
o This condition typically holds if $E(u|w) = 0$.
o Practical Result: We can often ignore that instruments were estimated when calculating 2SLS standard errors and test statistics.
6.1.3 Generated Instruments and Regressors
* If we have both generated regressors ($\hat{y}_2$) and instruments ($\hat{z}$), and we estimate:
$$y_i = x_i \beta + \gamma \hat{f}_i + error_i$$
o If testing $H_0: \gamma = 0$, the limiting distribution does not depend on the estimation of $\delta$.
o Therefore, the usual 2SLS $t$ statistic for $\hat{\gamma}$ is valid for testing $\gamma=0$.
6.2 Control Function Approach to Endogeneity
* Overview: Alternative to 2SLS, often useful for nonlinear models and testing endogeneity.
* Mechanism: Uses extra regressors (control functions) to break the correlation between endogenous variables and unobservables.
* Derivation:
o Model: $y_1 = z_1 \delta_1 + \alpha_1 y_2 + u_1$ (6.9) where $y_2$ is endogenous.
o Reduced Form for $y_2$: $y_2 = z \pi_2 + v_2$ (6.11).
o Project $u_1$ on reduced form error $v_2$:
$$u_1 = \rho_1 v_2 + e_1$$
* Where $\rho_1 = E(v_2 u_1)/E(v_2^2)$ and $e_1$ is uncorrelated with $v_2$ and $z$.
o Augmented Equation:
$$y_1 = z_1 \delta_1 + \alpha_1 y_2 + \rho_1 v_2 + e_1$$
* Since $e_1$ is uncorrelated with $z_1, y_2, v_2$, we can estimate this by OLS if we observe $v_2$.
* Implementation (Control Function Estimator):
1. Run OLS of $y_2$ on $z$ to get residuals $\hat{v}_2$.
2. Run OLS of $y_1$ on $z_1, y_2,$ and $\hat{v}_2$.
o This is the Control Function (CF) estimator.
* Properties & Comparison to 2SLS:
o In the linear model (6.9), CF estimates are identical to 2SLS estimates.
o Testing Endogeneity: CF leads to a straightforward test. The coefficient on $\hat{v}_2$ ($\rho_1$) captures the correlation between $u_1$ and $v_2$.
* Test $H_0: \rho_1 = 0$ using a standard $t$ test (robust if needed). This tests if $y_2$ is exogenous.
o Identification: Requires $z$ to contain at least one element not in $z_1$ (otherwise perfect collinearity between $\hat{v}_2$ and explanatory variables).
* Extension to Nonlinear Functions of Endogenous Variables:
o Consider model with $y_2$ and $y_2^2$:
$$y_1 = z_1 \delta_1 + \alpha_1 y_2 + \gamma_1 y_2^2 + u_1$$
.
o 2SLS Approach: Requires instruments for $y_2$ and $y_2^2$ (e.g., $z_1, z_2, z_2^2$).
o CF Approach:
* Requires stronger assumption: $E(u_1 | z, y_2) = \rho_1 v_2$ (Linearity of conditional expectation).
* Augmented Regression: $y_1$ on $z_1, y_2, y_2^2, \hat{v}_2$.
o Trade-off:
* 2SLS: Consistent under weaker assumptions (orthogonality), but less efficient.
* CF: Generally more precise (efficient) because it solves endogeneity by adding a scalar $\hat{v}_2$, but inconsistent if the linear conditional expectation assumption ($E(y_2|z) = z\pi_2$ and $E(u_1|v_2)$ is linear) fails.
Based on the timestamps of the uploaded images, the new section following the previous conversation (which ended at Section 6.2) is Section 6.3. Below is the reproduction of the notes from the images covering Section 6.3 and the relevant examples.
6.3 Some Specification Tests
* We move from classical hypothesis testing to "specification tests," which test the assumptions underlying the consistency of OLS or 2SLS.
6.3.1 Testing for Endogeneity
* Motivation: If all explanatory variables are exogenous, OLS is preferred over 2SLS due to efficiency (smaller variance). We need a test to determine if endogenous variables are actually present.
* The Durbin-Wu-Hausman (DWH) Test:
o Compares OLS and 2SLS estimates.
o Under the null hypothesis (all variables exogenous), the estimates differ only by sampling error.
o Test Statistic (Non-robust):
$$(\hat{\beta}_{2SLS} - \hat{\beta}_{OLS})' (\hat{V}_{2SLS} - \hat{V}_{OLS})^{-1} (\hat{\beta}_{2SLS} - \hat{\beta}_{OLS})$$
* Assumes homoskedasticity ($\sigma^2$ estimated usually using 2SLS variance).
o Issues: Computing the variance difference matrix is cumbersome and singular if fewer endogenous variables exist than parameters. A robust version is rarely computed in standard software.
* Regression-Based Tests (Hausman 1978, 1983):
o Asymptotically equivalent to DWH but easier to compute and make robust.
o Procedure for Single Endogenous Variable ($y_2$):
1. Estimate the reduced form for $y_2$:
$$y_2 = \mathbf{z}\pi_2 + v_2$$
2. Obtain the residuals $\hat{v}_2$.
3. Estimate the structural equation adding $\hat{v}_2$ as a regressor (OLS):
$$y_1 = z_1\delta_1 + \alpha_1 y_2 + \rho_1 \hat{v}_2 + error$$
4. Test $H_0: \rho_1 = 0$ using a $t$ statistic.
o Logic: If $\rho_1 \neq 0$, then $u_1$ is correlated with $v_2$, implying $y_2$ is endogenous.
o Inference:
* If $H_0$ is true, standard errors are valid.
* Can simply use heteroskedasticity-robust $t$ statistic to robustify the test.
* If rejected, one should use 2SLS.
* Example 6.1 (Testing for Endogeneity of Education):
o Reduced form residuals $\hat{v}_2$ obtained from regressing educ on all instruments.
o Included in wage equation: $t$ statistic on $\hat{v}_2$ is -1.65 ($p$-value .10).
o Evidence of endogeneity at 10% level; 2SLS might be preferred.
* Testing Multiple Endogenous Variables:
o Let $y_2$ be a $1 \times G_1$ vector.
o Obtain residuals $\hat{\mathbf{v}}_2$ from each reduced form.
o Augment structural equation with all residuals:
$$y_1 = \mathbf{z}_1 \delta_1 + \mathbf{y}_2 \alpha_1 + \hat{\mathbf{v}}_2 \rho_1 + error$$
o Test joint hypothesis $H_0: \mathbf{\rho}_1 = \mathbf{0}$ using $F$ test or robust Wald test.
o LM-type Test:
* Can use $NR_u^2$ from regressing residuals $\tilde{u}_1$ (from OLS on $z_1, y_2$) on $z_1, y_2, \hat{v}_2$.
* Example 6.2 (Endogeneity of Education in Wage Equation):
o Model: $\log(wage)$ on educ, black $\cdot$ educ, and controls.
o Instruments: nearc4 (college proximity) for educ; black $\cdot$ nearc4 for black $\cdot$ educ.
o Testing:
* Regress educ on all instruments $\rightarrow$ save residuals $\hat{v}_1$.
* Regress black $\cdot$ educ on all instruments $\rightarrow$ save residuals $\hat{v}_2$.
* Add $\hat{v}_1$ and $\hat{v}_2$ to OLS wage equation.
* Joint $F$ test yields $p$-value = 0.581. Cannot reject exogeneity.
o Comparison:
* OLS return to education: ~7.1% (non-black).
* 2SLS return to education: ~12.7%.
* While estimates differ, 2SLS standard errors are so large that the estimates are not statistically different.
* Testing Partial Exogeneity (Subsets):
o Tests if a subset of explanatory variables is exogenous while allowing others to be endogenous.
o Procedure:
1. Specify expanded model $y_1 = \mathbf{z}_1\delta_1 + \mathbf{y}_2\alpha_1 + \mathbf{y}_3\gamma_1 + u_1$.
2. Test $H_0: E(\mathbf{y}_3' u_1) = 0$ (variables in $\mathbf{y}_3$ are exogenous).
3. Augment equation with residuals $\hat{\mathbf{v}}_3$ (from reduced forms of $\mathbf{y}_3$).
4. Estimate by 2SLS (using instruments for $\mathbf{y}_2$, while $\mathbf{y}_3$ act as their own instruments).
5. Test significance of $\hat{\mathbf{v}}_3$ using Wald/F statistic.
6.3.2 Testing Overidentifying Restrictions
* Context: Used when we have more instruments than needed ($L > K$, or number of excluded instruments $M > G_1$).
* Goal: Test if the additional instruments are valid (uncorrelated with $u_1$).
* Sargan Test (Regression-Based):
o Maintains homoskedasticity assumption (2SLS.3) under the null.
o Procedure:
1. Estimate the structural equation by 2SLS and obtain residuals $\hat{u}_1$.
2. Regress $\hat{u}_1$ on all exogenous variables $\mathbf{z}$ (instruments).
3. Compute test statistic:
$$LM = N R_u^2$$
* Where $R_u^2$ is the R-squared from this regression.
o Distribution: Distributed as $\chi_{Q_1}^2$ where $Q_1 = L_2 - G_1$ (number of overidentifying restrictions).
o Interpretation:
* Rejection implies at least one instrument is endogenous (or model is misspecified).
* Does not tell us which instrument is invalid.
* If we fail to reject, it gives some confidence in the instrument set, but it is possible for multiple instruments to be invalid in a way that satisfies the test.
o Heteroskedasticity: A robust version is available (see Wooldridge 1995b).

Here is the reproduction of the new material found in the latest pictures (covering the end of Section 6.3.2 through Section 6.3.4).
6.3.2 Testing Overidentifying Restrictions (Continued)
* Example 6.3 (Overidentifying Restrictions in the Wage Equation):
o Model: Log wage equation using motheduc, fatheduc, and huseduc as instruments for educ.
o Test: With 3 instruments and 1 endogenous variable, there are 2 overidentifying restrictions ($Q_1 = 2$).
o Procedure:
1. Estimate 2SLS and get residuals $\hat{u}_1$.
2. Regress $\hat{u}_1$ on all exogenous variables (1, exper, exper$^2$, motheduc, fatheduc, huseduc).
3. Calculate $LM = N R_u^2$.
o Result: $R_u^2 = .0026$, $LM \approx 1.11$, $p$-value $\approx .574$.
o Conclusion: Overidentifying restrictions are not rejected.
o Caveat: Similarity in IV estimates (or failure to reject) doesn't prove exogeneity; instruments could share the same bias (e.g., all correlated with unobserved ability).
6.3.3 Testing Functional Form
* Goal: Test for neglected nonlinearities in OLS or 2SLS models.
* Approach: Add nonlinear functions (squares, cross-products) to the original model and test for significance using F or LM statistics.
o Note: For models with endogenous variables, instruments are needed for the added nonlinear terms (postponed to Chapter 9).
* RamseyÕs (1969) RESET:
o Conserves degrees of freedom compared to adding all cross-products.
o Procedure:
1. Obtain fitted values $\hat{y}_i = \mathbf{x}_i \hat{\beta}$ from the OLS regression.
2. Add low-order polynomials of fitted values (e.g., $\hat{y}_i^2, \hat{y}_i^3$) to the regression equation:
$$y = \mathbf{x}\beta + \delta_1 \hat{y}^2 + \delta_2 \hat{y}^3 + error$$
3. Test joint significance of the added terms using a standard $F$ test or $LM$ test ($N R^2$).
o Interpretation: Tests if the expected value of $y$ given the variables in the regression is linear in those variables.
o Limitation: RESET is generally a poor test for omitted variables or heteroskedasticity. It will not detect an omitted variable $q$ if $E(q|x)$ is linear in $x$.
* Example 6.4 (Testing for Neglected Nonlinearities in a Wage Equation):
o Null Hypothesis: $E(u|x) = 0$ (functional form is correct).
o Test: Adding $\hat{y}^2$ and $\hat{y}^3$ yields $R^2 = .0004$ and $\chi^2 = .374$ ($p$-value $\approx .83$).
o Result: No evidence of functional form misspecification.
o Note: RESET failed to detect the known omitted variable problem (IQ) because $IQ$ is likely linearly related to the regressors.
6.3.4 Testing for Heteroskedasticity
* Motivation: While heteroskedasticity doesn't affect consistency, we may want to test for it to justify using more efficient estimation (GLS) or robust inference.
* Assumption: We maintain $E(y|x)$ is correctly specified ($E(u|x)=0$) under $H_0$. If functional form is misspecified, a heteroskedasticity test might just detect that misspecification.
* General Test Procedure:
o Test $H_0: E(u^2|x) = \sigma^2$ against the alternative that $E(u^2|x)$ depends on $x$.
o Look at covariances $Cov(h(\mathbf{x}), u^2)$ for some vector function $h(\mathbf{x})$.
o Regression:
$$u_i^2 = \delta_0 + \mathbf{h}_i \delta + v_i$$
o Since $u_i$ is unobserved, replace with OLS residuals $\hat{u}_i$:
$$\hat{u}_i^2 \text{ on } 1, \mathbf{h}_i$$
o Test Statistic: $LM = N R_u^2$ (from the regression of squared residuals) is distributed as $\chi_Q^2$ where $Q$ is the dimension of $\mathbf{h}$.
o Homokurtosis Assumption: This standard LM test assumes constant conditional fourth moment ($E(u^4|x) = constant$) under $H_0$.
* Specific Tests (Choices of h):
1. Breusch-Pagan (Koenker's Version):
* Take $\mathbf{h}_i = \mathbf{x}_i$ (the regressors themselves).
* $Q = K$.
* Preferred over original Breusch-Pagan because it relies less on normality of $u$.
2. White's (1980b) Test:
* Take $\mathbf{h}_i$ to be all nonconstant unique elements of $\mathbf{x}_i$, their squares, and cross-products.
* Degrees of freedom ($Q$) can be very large.
3. Special Case of White Test:
* To conserve degrees of freedom, take $\mathbf{h}_i = (\hat{y}_i, \hat{y}_i^2)$.
* $Q = 2$.
* Regress $\hat{u}_i^2$ on $1, \hat{y}_i, \hat{y}_i^2$ and use $N R^2$.
* This detects heteroskedasticity correlated with the conditional mean.
* Heterokurtosis-Robust Version:
o Relaxing the homokurtosis assumption involves the regression:
$$1 \text{ on } (\mathbf{h}_i - \bar{\mathbf{h}})(\hat{u}_i^2 - \hat{\sigma}^2)$$
o The test statistic is $N - SSR_0$ from this regression (asymptotically $\chi_Q^2$).
* Testing for Heteroskedasticity with Endogenous Variables (2SLS):
o Assume we have instruments $\mathbf{z}$ such that $E(u|\mathbf{z})=0$.
o Test $H_0: E(u^2|\mathbf{z}) = \sigma^2$ (Assumption 2SLS.3).
o Procedure:
* Use 2SLS residuals $\hat{u}_i$.
* Regress $\hat{u}_i^2$ on functions of exogenous variables, $\mathbf{h}_i = h(\mathbf{z}_i)$.
o Crucial Caveat: $\mathbf{h}_i$ must only depend on exogenous variables and estimated parameters.
* It is invalid to use endogenous elements of $\mathbf{x}_i$ in $\mathbf{h}_i$.
* It is valid to use fitted values $\hat{\mathbf{x}}_i$ (first-stage fitted values) or $\hat{y}_i$ (based on instruments).
o Additional Assumption: For the standard regression-based tests to be valid, we need $Cov(\mathbf{x}_i, u_i | \mathbf{z}_i)$ to be constant under $H_0$. If this fails, more complicated tests (Wooldridge 1990) are needed.

Based on your request to avoid duplicating sections you already have and to focus on the new material found in the latest images, here is the reproduction of Section 6.4 and Section 6.4.2.
6.4 Correlated Random Coefficient Models
* Context: In Section 4.3.3, models were discussed where unobserved heterogeneity interacts with explanatory variables (random coefficient models).
* OLS Consistency: If heterogeneity is independent of covariates, OLS consistently estimates the Average Partial Effect (APE).
* The Problem: Consistency is harder if explanatory variables are endogenous and no proxy variables exist to break the correlation between unobservables and endogenous variables.
6.4.1 When Is the Usual IV Estimator Consistent?
* Model Specification:
$$y_1 = \eta_1 + z_1\delta_1 + a_1 y_2 + u_1$$
o $z_1$: exogenous variables ($1 \times L_1$).
o $y_2$: endogenous explanatory variable.
o $a_1$: random coefficient on $y_2$ (unobserved random variable).
o $z_1(d_1 - \delta_1)$: zero mean conditional on $z$, absorbed in error.
* Correlated Random Coefficient (CRC) Model:
o Write $a_1 = \alpha_1 + v_1$, where $\alpha_1 = E(a_1)$ is the object of interest (APE).
o Rewritten Equation:
$$y_1 = \eta_1 + z_1\delta_1 + \alpha_1 y_2 + v_1 y_2 + u_1$$
o Composite error: $e_1 = v_1 y_2 + u_1$.
o This shows an explicit interaction between unobserved heterogeneity $v_1$ and $y_2$.
* IV Estimation Issues:
o The problem with 2SLS is that the error term $v_1 y_2 + u_1$ is not necessarily uncorrelated with instruments $z$, even if $E(u_1|z)=0$ and $E(v_1|z)=0$.
o Allowing $y_2$ and $v_1$ to be correlated implies the composite error has a nonzero mean, affecting the intercept $\eta_1$, but this is rarely a primary concern.
o Inconsistency: The usual IV estimator is generally inconsistent if $E(v_1 y_2 | z)$ depends on $z$.
* Condition for Consistency:
o Sufficient condition for IV to be consistent for $\delta_1$ and $\alpha_1$:
$$Cov(v_1, y_2 | z) = Cov(v_1, y_2)$$
o Interpretation: The conditional covariance between the random coefficient heterogeneity $v_1$ and the endogenous variable $y_2$ must not be a function of the instruments $z$ (though the unconditional covariance is unrestricted).
o Plausibility:
* Holds if $y_2 = m_2(z) + v_2$ and $(v_1, v_2)$ is independent of $z$.
* independence of $z$ is a strong assumption; not suitable for discrete $y_2$.
* Extensions (Interactions):
o Model with interaction between exogenous variables $z_1$ and endogenous $y_2$:
$$y_1 = \eta_1 + z_1\delta_1 + \alpha_1 y_2 + (z_1 - \psi_1)y_2\gamma_1 + v_1 y_2 + u_1$$
o Subtracting the mean $\psi_1 = E(z_1)$ ensures $\alpha_1$ remains the APE.
o Estimation: Maintain condition (6.44). Replace $\psi_1$ with sample averages.
o Instruments: Use interactions between $z_{i1}$ and all elements of $z_i$. Alternatively, use fitted values from first stage for $y_2$ and $(z_{i1} - \bar{z}_1)\hat{y}_{i2}$ as instruments.
o Heckman and Vytlacil (1998) suggest a two-step procedure using predicted values as regressors, but standard IV methods on equation (6.47) provide consistent APE estimators without adjusting standard errors for generated regressors.
6.4.2 Control Function Approach
* Overview: Garen (1984) proposed a control function approach for the correlated random coefficient model, which is instructive to contrast with IV.
* Assumptions:
o Assumes a specific reduced form: $y_2 = z\pi_2 + v_2$.
o Assumes $(u_1, v_1, v_2)$ are independent of $z$ with mean-zero trivariate normal distribution (stronger than needed).
o We can get by with linearity assumptions:
$$E(u_1 | z, v_2) = \rho_1 v_2, \quad E(v_1 | z, v_2) = \xi_1 v_2$$
* Derivation:
o Substitute expectations into the main equation:
$$E(y_1 | z, y_2) = \eta_1 + z_1\delta_1 + \alpha_1 y_2 + E(v_1 | z, y_2)y_2 + E(u_1 | z, y_2)$$
$$= \eta_1 + z_1\delta_1 + \alpha_1 y_2 + \xi_1 v_2 y_2 + \rho_1 v_2$$
* Estimation Procedure:
1. Regress $y_2$ on $z$ to obtain reduced-form residuals $\hat{v}_2$.
2. Run OLS regression of $y_1$ on $1, z_1, y_2, \hat{v}_2 y_2, \hat{v}_2$.
* Properties:
o Consistently estimates $\delta_1$ and $\alpha_1$ under the maintained assumptions.
o Requires adjustment of standard errors due to generated regressors ($\hat{v}_2$).
o Test for Exogeneity: Simple $F$ test of $H_0: \xi_1 = 0, \rho_1 = 0$. Under the null, no standard error adjustment is needed.
o Trade-off:
* Garen's CF estimator is likely more efficient than IV if assumptions hold.
* However, it relies on more restrictive assumptions (linear model for $E(y_2|z)$ and linearity of error expectations) than the standard IV approach.

Based on the new pictures provided, here is the reproduction of Section 6.5.
6.5 Pooled Cross Sections and Difference-in-Differences Estimation
* Moves from random samples to data structures exploiting random samples from different points in time.
6.5.1 Pooled Cross Sections over Time
* Definition: A new random sample is taken from the relevant population in each time period.
o Assumption: Independent, not identically distributed (i.n.i.d.) observations (distributions change over time).
o Contrast with Panel Data: Panel data follows the same units (individuals, firms) over time; pooled cross sections do not (recurrence of units is coincidental and ignored).
* Methodology:
o All previous methods (OLS, 2SLS, heteroskedasticity corrections) apply.
o Time Dummies: usually included to account for aggregate changes (inflation, productivity).
o 2SLS Context: Time dummies act as their own instruments because the passage of time is exogenous.
* Interactions:
o Interacting explanatory variables with time dummies allows partial effects (e.g., return to schooling, gender gap) to change over time.
6.5.2 Policy Analysis and Difference-in-Differences Estimation
* Setup (Natural Experiments):
o Two time periods: Year 1 (pre-policy) and Year 2 (post-policy).
o Two groups:
* Treatment Group ($A$): Subject to policy change (e.g., state with increased benefits).
* Control Group ($B$): Not subject to change (e.g., neighboring state, unaffected group).
* Model:
$$y = \beta_0 + \beta_1 dB + \delta_0 d2 + \delta_1 d2 \cdot dB + u$$
o $dB$: Dummy for treatment group (captures pre-existing differences).
o $d2$: Dummy for second time period (captures aggregate time trends).
o $d2 \cdot dB$: Interaction term (policy dummy).
o $\delta_1$: Coefficient of interest (measure of policy effect).
* The DD Estimator:
$$\hat{\delta}_1 = (\bar{y}_{B,2} - \bar{y}_{B,1}) - (\bar{y}_{A,2} - \bar{y}_{A,1})$$
o Labeled Difference-in-Differences (DD) estimator.
o Interpretation: Compares the time change in means for the treatment group to the time change in means for the control group, differencing out group-specific and time-specific effects.
o Requirement: Policy change must not be systematically related to other factors hidden in $u$.
* Example 6.5 (Length of Time on Workers' Compensation):
o Policy: Kentucky raised earnings cap for workers' compensation.
o Groups:
* Treatment: High-income workers (constraint binding).
* Control: Low-income workers (unaffected by cap increase).
o Result: $\hat{\delta}_1 = .191$ ($t=2.77$). Duration on compensation increased ~19% for high earners due to the policy.
o Controls: Adding demographics/industry dummies (MVD study) had little effect on $\hat{\delta}_1$, suggesting random sampling held.
* Extensions:
o Continuous Treatments: e.g., distance from incinerator. Replace treatment dummy $dB$ with continuous variable.
o Alternative Control Groups:
* Use unaffected age groups (e.g., <65 vs >65 for elderly health policy) within the same state.
* Problem: "Changes" in health might differ systematically by age regardless of policy.
* Difference-in-Difference-in-Differences (DDD):
o Motivation: Standard DD fails if trends differ between treatment and control groups.
o Strategy: Use a control group within the treatment state ($E$) alongside the control state ($A$).
o Model:
$$y = \beta_0 + \beta_1 dB + \beta_2 dE + \beta_3 dB \cdot dE + \delta_0 d2 + \delta_1 d2 \cdot dB + \delta_2 d2 \cdot dE + \delta_3 d2 \cdot dB \cdot dE + u$$
o DDD Estimator ($\hat{\delta}_3$):
$$\hat{\delta}_3 = [(\bar{y}_{B,E,2} - \bar{y}_{B,E,1}) - (\bar{y}_{B,N,2} - \bar{y}_{B,N,1})] - [(\bar{y}_{A,E,2} - \bar{y}_{A,E,1}) - (\bar{y}_{A,N,2} - \bar{y}_{A,N,1})]$$
* The second term in brackets accounts for the differential trend in the absence of policy (using state A).
* Multiple Time Periods:
o Add full set of time period dummies.
o Policy dummy is unity for groups/periods subject to policy.
o For DDD: Include full set of dummies for groups, time periods, and pairwise interactions. Policy variable measures effect.
* Application (Carpenter 2004):
o Analyzed state zero-tolerance alcohol laws for drivers under 21.
o Design: DDD-type.
o Controls: State dummies, year dummies, month dummies (seasonality).
o Interaction: zerotol $\cdot$ under21. Coefficient measures policy effect.
13 Maximum Likelihood Methods
13.1 Introduction
* Context: Previous chapters focused on zero-covariance and zero-conditional-mean assumptions (OLS, 2SLS) without full distributional assumptions.
* Role of MLE:
o Used when we specify the full distribution of endogenous variables conditional on exogenous variables.
o Efficiency: Generally the most efficient estimation procedure if the distribution is correctly specified (Theorem 14.4).
o Risk: Inconsistent if any part of the specified distribution is misspecified (nonrobust).
* Comparison:
o SEM: MLE is efficient if errors are multivariate normal, but inconsistent if normality fails (unlike GMM).
o Bounded Dependent Variable: For $y \in [0,1]$, NLS is consistent for $E(y|x)$ and robust. MLE (modeling the density) estimates any feature like $P(y=1|x)$ but is inconsistent for the mean if the density is wrong.
13.2 Preliminaries and Examples
* Framework:
o Focus is on Conditional Maximum Likelihood Estimation (CMLE).
o We estimate the conditional density of $y_i$ given $\mathbf{x}_i$, denoted $f(y|\mathbf{x}; \theta)$.
o $\mathbf{x}_i$ are not treated as fixed constants (too restrictive); we simply condition on them.
o Applies to random sampling, system estimation (Chapters 7, 9), and some panel data settings.
* Example 13.1 (Probit):
o Latent Variable Model:
$$y_i^* = \mathbf{x}_i\theta + e_i$$
* $e_i$ is independent of $\mathbf{x}_i$ and $e_i \sim Normal(0,1)$.
o Observed Variable:
$$y_i = 1 \text{ if } y_i^* > 0$$
$$y_i = 0 \text{ if } y_i^* \le 0$$
o Conditional Probabilities:
$$P(y_i = 1 | \mathbf{x}_i) = P(e_i > -\mathbf{x}_i\theta) = 1 - \Phi(-\mathbf{x}_i\theta) = \Phi(\mathbf{x}_i\theta)$$
* Where $\Phi(\cdot)$ is the standard normal cdf.
o Density:
$$f(y|\mathbf{x}_i) = [\Phi(\mathbf{x}_i\theta)]^y [1 - \Phi(\mathbf{x}_i\theta)]^{1-y}, \quad y=0,1$$
* Count Data Models:
o Used when $y$ takes nonnegative integer values (0, 1, 2...).
o Linear model for $E(y|x)$ is not ideal (can predict negative values).
o Exponential function suited for conditional mean: $E(y|x) = \exp(\mathbf{x}\theta)$.
o Can be estimated by NLS, but standard count distributions imply heteroskedasticity, suggesting MLE is better.
* Example 13.2 (Poisson Regression):
o Assumes $y_i$ given $\mathbf{x}_i$ follows a Poisson distribution with mean $\mu(\mathbf{x}_i) = \exp(\mathbf{x}_i\beta)$.
o Density:
$$f(y|\mathbf{x}_i) = \exp[-\mu(\mathbf{x}_i)] [\mu(\mathbf{x}_i)]^y / y!$$
o Property: Variance equals the mean: $Var(y_i|\mathbf{x}_i) = E(y_i|\mathbf{x}_i)$.
13.3 General Framework for Conditional Maximum Likelihood Estimation
* Setup:
o $p_o(y|\mathbf{x})$: True conditional density.
o $f(y|\mathbf{x}; \theta)$: Parametric model density.
o Assumption: Correct specification. There exists $\theta_o$ such that $f(\cdot|\mathbf{x}; \theta_o) = p_o(\cdot|\mathbf{x})$.
* Analogy Principle:
o Based on the conditional Kullback-Leibler information inequality:
$$E[\log(p_o(y|\mathbf{x}) / f(y|\mathbf{x}; \theta)) | \mathbf{x}] \ge 0$$
o The inequality is minimized (value 0) when $f = p_o$, implying $\theta = \theta_o$.
o This motivates maximizing the expected log likelihood:
$$\max_{\theta \in \Theta} E[\ell_i(\theta)]$$
* Where $\ell_i(\theta) = \log f(y_i|\mathbf{x}_i; \theta)$ is the conditional log likelihood for observation $i$.
* The Estimator (CMLE):
o The sample analogue maximizes the sum of log likelihoods:
$$\max_{\theta \in \Theta} N^{-1} \sum_{i=1}^N \log f(y_i|\mathbf{x}_i; \theta)$$
o Solution $\hat{\theta}$ is the Conditional Maximum Likelihood Estimator.
o It is an M-estimator (can be viewed as minimizing negative log likelihood).
* Joint vs. Conditional MLE:
o Under independence, the joint density is the product of conditional densities (marginal of $\mathbf{x}$ is ignored/free of $\theta$).
o Maximizing the product of densities is equivalent to maximizing the sum of log densities.
* Log Likelihood Functions:
o Probit (Ex 13.1):
$$\ell_i(\theta) = y_i \log \Phi(\mathbf{x}_i\theta) + (1 - y_i) \log [1 - \Phi(\mathbf{x}_i\theta)]$$
o Poisson (Ex 13.2):
$$\ell_i(\beta) = -\exp(\mathbf{x}_i\beta) + y_i \mathbf{x}_i\beta - \log(y_i!)$$
* The term $\log(y_i!)$ is usually dropped as it does not affect maximization.
- Consistency of Estimator:
o The conditional maximum likelihood estimator (CMLE) $\hat{\theta}$ is the solution to maximizing the sum of conditional log likelihoods.
o It is an M-estimator, motivated by the analogy principle that minimizes the Kullback-Leibler divergence between the true density and the model density.
o Note on Maximization: We often define the objective function with a minus sign (minimization) to apply general M-estimator theorems, but for MLE, we naturally talk about maximization.
o Independence: Under independence, the joint log-likelihood is the sum of the conditional log-likelihoods (marginal distribution of $\mathbf{x}$ is ignored as it does not depend on $\theta$).
13.4 Consistency of Conditional Maximum Likelihood Estimation
* THEOREM 13.1 (Consistency of CMLE):
o Let $\{(x_i, y_i): i=1,2,...\}$ be a random sample.
o Let $\Theta$ be the parameter set.
o Assumptions:
* (a) $f(\cdot | \mathbf{x}; \theta)$ is a true density with respect to measure $v(dy)$ (conditions hold).
* (b) Identification: For some $\theta_0 \in \Theta$, $p_0(\cdot|\mathbf{x}) = f(\cdot|\mathbf{x}; \theta_0)$. $\theta_0$ is the unique solution to the maximization problem.
* (c) $\Theta$ is a compact set.
* (d) Log-likelihood is measurable.
* (e) Log-likelihood is continuous on $\Theta$.
* (f) Log-likelihood is bounded by an integrable function (dominance condition).
o Result: The CMLE $\hat{\theta}$ exists and $\text{plim } \hat{\theta} = \theta_0$.
* Discussion of Assumptions:
o Compactness (c) and measurability (d) are technical.
o Continuity (e) usually holds but can fail in specific auction models (Donald and Paarsch 1996).
o Key Concerns:
1. Correct Specification: The density must be correct. e.g., Probit requires normality and independence of latent error $e$. Poisson often works even if distributional assumptions fail (robustness discussed later).
2. Identification: Must rule out perfect collinearity in $\mathbf{x}$.
13.5 Asymptotic Normality and Asymptotic Variance Estimation
* Overview: Under differentiability and moment assumptions (Theorem 12.3), MLE is asymptotically normal.
13.5.1 Asymptotic Normality
* Score Function:
o The score for observation $i$ is the vector of partial derivatives of the log-likelihood:
$$s_i(\theta) = \nabla_\theta \ell_i(\theta)$$
o Example 13.1 (Probit Score):
$$s_i(\theta) = \frac{\phi(\mathbf{x}_i\theta) \mathbf{x}_i' [y_i - \Phi(\mathbf{x}_i\theta)]}{\Phi(\mathbf{x}_i\theta)[1 - \Phi(\mathbf{x}_i\theta)]}$$
o Example 13.2 (Poisson Score):
$$s_i(\theta) = \mathbf{x}_i' [y_i - \exp(\mathbf{x}_i\theta)]$$
* Zero Conditional Mean of Score:
o A key property of the score at the true parameter $\theta_0$ is:
$$E[s_i(\theta_0) | \mathbf{x}_i] = 0$$
o This holds generally if integration and differentiation can be interchanged (regularity condition).
o Implies unconditional mean is zero: $E[s_i(\theta_0)] = 0$.
* Hessian and Information Matrix:
o Hessian ($H_i$): Matrix of second partial derivatives of $\ell_i(\theta)$.
o Expected Hessian ($A_0$):
$$A_0 = -E[H_i(\theta_0)]$$
* Generally positive definite.
o Information Matrix Equality (CIME/UIME):
* Under correct specification:
$$-E[H_i(\theta_0) | \mathbf{x}_i] = Var(s_i(\theta_0) | \mathbf{x}_i) = E[s_i(\theta_0) s_i(\theta_0)' | \mathbf{x}_i]$$
* Taking expectations gives $A_0 = B_0$, where $B_0 = E[s_i(\theta_0) s_i(\theta_0)']$.
* THEOREM 13.2 (Asymptotic Normality of CMLE):
o Under regularity conditions (smoothness, bounded derivatives, positive definite $A_0$):
$$\sqrt{N}(\hat{\theta} - \theta_0) \xrightarrow{d} Normal(0, A_0^{-1})$$
o Asymptotic Variance:
$$Avar(\hat{\theta}) = A_0^{-1} / N$$
13.5.2 Estimating the Asymptotic Variance
* Estimators for $A_0$:
Since $A_0 = B_0$ (Information Matrix Equality), we have three consistent estimators for $Avar(\hat{\theta})$:
1. Hessian Form: Based on second derivatives ($-\sum H_i$). Usually positive definite.
2. Outer Product of the Score (OPG): Based on first derivatives ($\sum s_i s_i'$). (Berndt, Hall, Hall, Hausman - BHHH).
* Simple to compute but can be poorly behaved in small samples.
3. Expected Hessian Form: Based on conditional expectation of Hessian ($\sum A(\mathbf{x}_i, \hat{\theta})$).
* Often has better finite sample properties and is positive definite.
* Example Estimates:
o Probit: $Avar(\hat{\theta})$ uses the Expected Hessian form (weights depend on $\phi$ and $\Phi$).
o Poisson: Hessian and Expected Hessian are identical (depend on $\exp(\mathbf{x}\beta)$).
13.6 Hypothesis Testing
* Likelihood Ratio (LR) Statistic:
$$LR = 2[\mathcal{L}(\hat{\theta}) - \mathcal{L}(\tilde{\theta})]$$
o Distributed $\chi_Q^2$ under $H_0$. $\tilde{\theta}$ is the restricted estimator.
o Invariant to reparameterization.
* Wald Statistic:
o Uses unrestricted estimates. Standard quadratic form.
* Score (LM) Statistic:
o Uses restricted estimates $\tilde{\theta}$.
o Computes score vector at restricted estimates.
o Various forms based on variance estimator (Hessian vs. OPG).
o Outer Product Form:
$$LM = (\sum \tilde{s}_i)' (\sum \tilde{s}_i \tilde{s}_i')^{-1} (\sum \tilde{s}_i)$$
* Invariant to reparameterization (unlike Hessian form).
o Can often be computed via auxiliary regressions using weighted residuals (e.g., for Probit/Poisson).
Here is the reproduction of the content from the latest batch of images (Section 13.7 and Section 13.8).
13.7 Specification Testing
* Motivation: Since MLE relies on distributional assumptions, we need general specification tests.
* General Approach: Nest the model of interest within a general model and use a score test (like RESET in linear models).
* Moment Conditions Test:
o Test if specific features (moments, probabilities) match the model.
o Let $w_i = (x_i, y_i)$. Test condition:
$$H_0: E[g(w_i, \theta_0)] = 0$$
o $g(w, \theta)$ is a $Q \times 1$ vector chosen to test features of interest (e.g., first or second conditional moments).
o Constraint: $g$ cannot contain elements of the score $s(w, \theta)$ because the score sums to zero by definition.
* Test Statistic Derivation:
o Based on how far the sample average $N^{-1} \sum g(w_i, \hat{\theta})$ is from zero.
o The asymptotic variance accounts for estimation of $\theta$:
$$N^{-1/2} \sum [g_i(\hat{\theta}) - \Pi_0 s_i(\hat{\theta})] = N^{-1/2} \sum g_i(\hat{\theta})$$
* Because $\sum s_i(\hat{\theta}) = 0$.
o $\Pi_0$ is the population regression coefficient matrix of $g_i$ on $s_i$.
o Conditional Information Matrix Equality (CIME): Under correct specification, $E[\nabla_\theta g_i(\theta_0) | x_i] = E[g_i(\theta_0)s_i(\theta_0)' | x_i]$. This simplifies the variance.
* Newey-Tauchen-White (NTW) Statistic:
o A quadratic form based on the standardized partial sums:
$$NTW = \left[ \sum_{i=1}^N \hat{g}_i \right]' \left[ \sum_{i=1}^N (\hat{g}_i - \hat{\Pi} \hat{s}_i)(\hat{g}_i - \hat{\Pi} \hat{s}_i)' \right]^{-1} \left[ \sum_{i=1}^N \hat{g}_i \right]$$
o Distributed asymptotically as $\chi_Q^2$ under $H_0$.
o Computation: Can be calculated as $N - SSR_0 = NR_u^2$ from the regression:
$$1 \text{ on } \hat{s}_i', \hat{g}_i'$$
* Warning: The outer product form (regression) can have poor finite-sample properties. Expected Hessian forms are often better.
* Applications:
o Information Matrix (IM) Test (White 1982a): Tests if the variance of the score equals the negative expected Hessian.
o Conditional Moment Tests: Test orthogonality conditions based on conditional moments (e.g., $E(u|x)=0, Var(u|x)=\sigma^2$).
o Example (Poisson): Test if $E(y|x) = \exp(x\theta_0)$ holds. If true, $u = y - \exp(x\theta_0)$ has zero conditional mean.
13.8 Partial (or Pooled) Likelihood Methods for Panel Data
* Motivation: Sometimes we want to estimate structural parameters without specifying the full conditional density of $y$ given $x$ (e.g., due to complexity or robustness).
13.8.1 Setup for Panel Data
* Notation:
o $y$: $T \times 1$ vector for each cross-section unit.
o $x$: Vector containing all observable variables (covariates, lags, etc.).
* Assumption: We have a correctly specified model for the density of $y_t$ given $x_t$ for each time period $t$:
$$f_t(y_t | x_t; \theta), \quad t=1, 2, ..., T$$
.
* Partial Likelihood Distinction: We do not assume that the joint density is the product of these marginal densities:
$$\prod_{t=1}^T f_t(y_t | x_t; \theta)$$
o Is likely not the true joint density $D(y|x)$ because of serial correlation or other dependencies.
* The Estimator (PMLE):
o Partial Log Likelihood: Sum of log likelihoods across $t$:
$$\ell_i(\theta) = \sum_{t=1}^T \log f_t(y_{it} | x_{it}; \theta)$$
o Pooled Maximum Likelihood Estimator (PMLE): Maximizes $\sum_{i=1}^N \ell_i(\theta)$.
o Consistency: General M-estimator theory (Theorem 12.2) implies consistency if $\theta_0$ is identified (uniquely maximizes expectation).
o Inference: Because it ignores serial correlation (doesn't use true joint density), the Information Matrix Equality (CIME) does not hold. We need a robust variance estimator (Cluster-Robust Standard Errors).
* Strict Exogeneity:
o The joint density product $\prod p(y_t|z_t)$ generally requires strict exogeneity:
$$p(y_t | z_1, ..., z_T) = p(y_t | z_t)$$
o This fails if $x_t$ contains lagged dependent variables ($y_{t-1}$) or if feedback exists from $y$ to future $z$.
o Partial MLE Advantage: Does not necessarily require strict exogeneity, only that the specified marginal density $f_t(y_t|x_t)$ is correct.
* Example 13.3 (Probit with Panel Data):
o Model: Latent variable $y_{it}^* = x_{it}\theta + e_{it}$.
o Pooled Probit Estimator: Maximizes $\sum \sum [y_{it} \log \Phi(x_{it}\theta) + (1-y_{it})\log(1-\Phi(x_{it}\theta))]$.
o Consistency: Consistent for $\theta$ if $e_{it}$ is independent of $x_{it}$ and $e_{it} \sim Normal(0,1)$.
o Robustness:
* Works without assuming serial independence of $e_{it}$ (allows arbitrary correlation across $t$).
* Works with lagged dependent variables in $x_{it}$ (dynamic models), provided the model for $P(y_t=1|x_t)$ is correct.
* Standard errors must be adjusted for serial correlation (clustering).
Here is the reproduction of the content from the latest batch of images, covering Sections 13.8.2, 13.8.3, and 13.9.
13.8.2 Asymptotic Inference
* Motivation:
o Partial MLE is analogous to pooled OLS with serially correlated errors.
o Standard econometrics packages can compute partial MLEs (e.g., pooled probit) but often report invalid standard errors and test statistics because they ignore serial dependence.
* Asymptotic Variance:
o Combining M-estimation results, we have $Avar(\hat{\theta}) = A_0^{-1} B_0 A_0^{-1}$.
o $A_0$ (Hessian term): Sum of expected Hessians across time:
$$A_0 = \sum_{t=1}^T E[A_{it}(\theta_0)]$$
o $B_0$ (Score variance): Variance of the sum of scores (accounts for correlation):
$$B_0 = E \left[ \left( \sum_{t=1}^T s_{it}(\theta_0) \right) \left( \sum_{t=1}^T s_{it}(\theta_0) \right)' \right]$$
.
* Estimators:
o $\hat{A}$: $\sum_{i=1}^N \sum_{t=1}^T A_{it}(\hat{\theta})$ (or using observed Hessian).
o $\hat{B}$: $\sum_{i=1}^N \hat{s}_i \hat{s}_i'$ where $\hat{s}_i = \sum_{t=1}^T s_{it}(\hat{\theta})$ is the total score for unit $i$.
o Robust Variance: $\hat{A}^{-1} \hat{B} \hat{A}^{-1}$.
* This is the "cluster-robust" variance estimator often available in software.
* Inference:
o Wald Tests: Use the robust variance matrix.
o Robust Score (LM) Statistic: Valid (discussed in Section 12.6.2).
o Likelihood Ratio (LR) Statistic: Invalid because $B_0 \neq A_0$ generally (Information Matrix Equality fails).
* Special Case: If scores are serially uncorrelated, $B_0 = A_0$, and standard MLE statistics (including LR) are valid.
13.8.3 Inference with Dynamically Complete Models
* Definition: A conditional density is dynamically complete if it captures all dynamics:
$$f_t(y_t | \mathbf{x}_t; \theta) = p(y_t | \mathbf{x}_t, y_{t-1}, y_{t-2}, ...)$$
o Equivalently, $D(y_{it} | \mathbf{x}_{it}, y_{i,t-1}, ...) = D(y_{it} | \mathbf{x}_{it})$.
* Implication for Scores:
o Dynamic completeness implies the score is a Martingale Difference Sequence (MDS):
$$E[s_{it}(\theta_0) | \mathbf{x}_{it}, y_{i,t-1}, ...] = 0$$
o Therefore, scores are serially uncorrelated ($E[s_{it} s_{ir}'] = 0$ for $t \neq r$).
* Result:
o If the model is dynamically complete, $B_0 = A_0$.
o Standard (non-robust) MLE statistics are asymptotically valid. No need for clustering/robust SEs.
* Relationship to Strict Exogeneity:
o Dynamic completeness does not require strict exogeneity of $z_t$.
o It holds more easily when $\mathbf{x}_t$ contains lagged dependent variables ($y_{t-1}$).
o Test for Dynamic Completeness: Can test if scores are serially correlated (White 1994).
13.9 Panel Data Models with Unobserved Effects
* Introduces nonlinear unobserved effects models (covered in detail in Chapters 15-18).
13.9.1 Models with Strictly Exogenous Explanatory Variables
* Setup:
o Interest lies in the distribution of $y_{it}$ given observable $\mathbf{x}_{it}$ and unobserved heterogeneity $c_i$.
o Strict Exogeneity Assumption:
$$D(y_{it} | \mathbf{x}_{i1}, ..., \mathbf{x}_{iT}, c_i) = D(y_{it} | \mathbf{x}_{it}, c_i)$$
* Conditioning on contemporaneous $\mathbf{x}_{it}$ and $c_i$ is sufficient.
* Rules out lagged dependent variables and feedback from $y$ to future $x$.
* Approaches to Estimation:
1. Random Effects (RE):
* Assumes $c_i$ is independent of $\mathbf{x}_i$: $D(c_i | \mathbf{x}_i) = D(c_i)$.
* Restrictive but allows consistent estimation.
2. Fixed Effects (FE):
* No assumption on $D(c_i | \mathbf{x}_i)$.
* Treating $c_i$ as parameters to estimate leads to the incidental parameters problem (inconsistent $\theta_0$ with fixed $T$).
* "Fixed Effects" in this book refers to methods that eliminate $c_i$ from the conditional distribution (conditional MLE).
3. Correlated Random Effects (CRE):
* Middle ground: Model $D(c_i | \mathbf{x}_i)$ parametrically.
* Common restriction: $D(c_i | \mathbf{x}_i) = D(c_i | \bar{\mathbf{x}}_i)$ (Chamberlain/Mundlak approach using time averages).
* Estimation via Integration (RE/CRE):
o Let $h(c | \mathbf{x}; \delta)$ be the density of $c$.
o Assume conditional independence of $y_{it}$ given $(\mathbf{x}_i, c_i)$.
o Joint Density (Integrating out c):
$$f(\mathbf{y}_i | \mathbf{x}_i) = \int_{\mathbb{R}^J} \left[ \prod_{t=1}^T f_t(y_{it} | \mathbf{x}_{it}, c; \theta) \right] h(c | \mathbf{x}_i; \delta) dc$$
.
o Maximize the log of this joint density to estimate $\theta$ and $\delta$ (Conditional MLE).
* Alternative: Partial MLE with Integration:
o Obtain density of single $y_{it}$ by integrating out $c$:
$$f(y_{it} | \mathbf{x}_{it}) = \int f_t(y_{it} | \mathbf{x}_{it}, c; \theta) h(c | \mathbf{x}_i; \delta) dc$$
o Maximize sum of these partial log-likelihoods:
$$\sum_{i=1}^N \sum_{t=1}^T \log f(y_{it} | \mathbf{x}_{it})$$
.
o Inference: Scores will be serially correlated (due to common $c_i$), so robust inference is required.



13.9.2 Models with Lagged Dependent Variables
* Model Specification:
o We model $D(y_{it} | z_{it}, y_{i,t-1}, ..., y_{i0}, c_i)$.
o For simplicity, include only contemporaneous $z_{it}$ and one lag $y_{it-1}$.
o Assumption: Dynamics are correctly specified and $z_{it}$ is strictly exogenous conditional on $c_i$:
$$D(y_{it} | z_{it}, y_{i,t-1}, ..., y_{i0}, c_i) = D(y_{it} | z_{it}, y_{i,t-1}, c_i)$$
* Density:
o Given strict exogeneity and dynamic completeness, the density of $(y_{i1}, ..., y_{iT})$ given $(z_i, y_{i0}, c_i)$ is:
$$\prod_{t=1}^T f_t(y_{it} | z_{it}, y_{i,t-1}, c; \theta_0)$$
* Estimation Strategy (Conditioning on Initial Value):
o To estimate $\theta_0$, we integrate $c$ out of the density.
o We specify a density for $c$ given $z_i$ and the initial condition $y_{i0}$, denoted $h(c | z, y_{i0}; \delta)$.
o Conditional Log-Likelihood:
$$\log \left\{ \int_{\mathbb{R}^J} \left[ \prod_{t=1}^T f_t(y_{it} | z_{it}, y_{i,t-1}, c; \theta) \right] h(c | z_i, y_{i0}; \delta) dc \right\}$$
o Maximizing this with respect to $\theta$ and $\delta$ yields consistent and $\sqrt{N}$-asymptotically normal CMLEs.
* Trade-offs:
o Efficiency: Conditioning on $y_{i0}$ sacrifices efficiency compared to modeling $D(y_{i0}|z_i)$, but finding the latter distribution is extremely difficult.
o Fixed Effects vs. Random Effects:
* Treating $c_i$ as parameters to estimate (Fixed Effects) leads to the incidental parameters problem, causing inconsistent estimation of $\theta_0$ with fixed $T$.
* The CMLE approach (Random/Correlated Random Effects) requires specifying a density for $c_i$ but ensures consistency.
* Quantities of Interest:
o Since $c_i$ is unobserved, we typically want Average Partial Effects (APEs), which average out $c$.
o Wooldridge (2005b) shows APEs are generally identified under these assumptions.
13.10 Two-Step Estimators Involving Maximum Likelihood
13.10.1 Second-Step Estimator Is Maximum Likelihood Estimator
* Setup:
o Model $f(y | x; \theta, \gamma_0)$.
o $\hat{\gamma}$ is a preliminary estimator from a first stage.
o $\hat{\theta}$ solves $\max_{\theta} \sum \log f(y_i | x_i; \theta, \hat{\gamma})$.
* Properties:
o Consistency follows from standard two-step M-estimator results.
o Asymptotic variance of $\hat{\theta}$ generally depends on the asymptotic variance of $\hat{\gamma}$.
o Exception: Estimation of $\gamma$ can be ignored if the expected Hessian (w.r.t $\theta$ and $\gamma$) is block diagonal.
13.10.2 Surprising Efficiency Result When First-Step Estimator Is CMLE
* Scenario:
o Second-step M-estimator $\hat{\theta}$ solves $\min \sum q(v_i, w_i, z_i, \theta, \hat{\gamma})$.
o First-step estimator $\hat{\gamma}$ is a CMLE based on density $h(\cdot | z; \gamma)$.
* Assumption 13.66 (Conditional Independence):
$$D(v_i | w_i, z_i) = D(v_i | z_i)$$
o Meaning $z_i$ explains $v_i$ so well that $w_i$ provides no additional information.
* Result:
o Under Assumption 13.66, the two-step estimator is more efficient (has smaller asymptotic variance) than the one-step estimator that uses the true $\gamma_0$.
o The term $B_0 - D_0$ is positive semi-definite (where $D_0$ is variance reduction due to estimation of $\gamma$).
* Implication for Inference:
o If we ignore the first-step estimation of $\gamma$, our standard errors will be conservative (larger than necessary).
o Standard errors reflecting the increased precision can be computed using a sandwich estimator involving scores from both stages.
13.11 Quasi-Maximum Likelihood Estimation
* Overview: Used when we suspect the specified distributional density is misspecified (e.g., assuming normality when data is not normal).
13.11.1 General Misspecification
* Concept:
o There is no "true" $\theta_0$ in the parametric model.
o We estimate $\theta^*$, the parameter vector that minimizes the Kullback-Leibler information criterion (distance between true density and model density).
o The estimator $\hat{\theta}$ is called the Quasi-Maximum Likelihood Estimator (QMLE) or Pseudo-MLE.
* Asymptotic Variance:
o Since the Information Matrix Equality generally fails under misspecification ($A_0 \neq B_0$), we must use the sandwich estimator:
$$Avar(\hat{\theta}) = A(\theta^*)^{-1} B(\theta^*) A(\theta^*)^{-1}$$
o Estimated using:
$$\widehat{Avar}(\hat{\theta}) = \left( \sum_{i=1}^N H_i(\hat{\theta}) \right)^{-1} \left( \sum_{i=1}^N s_i(\hat{\theta}) s_i(\hat{\theta})' \right) \left( \sum_{i=1}^N H_i(\hat{\theta}) \right)^{-1}$$
* Inference:
o Wald Tests: Valid if using the fully robust variance matrix (sandwich form).
o Score (LM) Tests: Must use the fully robust framework. The expected Hessian cannot be used; must use the observed Hessian. Note that the statistic is not guaranteed to be nonnegative.
o LR Statistic: Invalid and not advised. Its distribution depends on unknown parameters.
* Example (Probit QMLE):
o If the probit model is misspecified, $\hat{\theta}$ converges to $\theta^*$, providing the best approximation to $P(y=1|x)$.
o We can still estimate the partial effect of $x_j$ on the probability using $\theta^*$.
o Robust inference (sandwich estimator) is required for confidence intervals.
Based on the new images provided in your upload (specifically covering Sections 13.11.2 and 13.11.3), here is the reproduction of that content.
13.11.2 Model Selection Tests
* Overview:
o Used to choose between competing models, derived from MLE properties under misspecification.
o Focuses on nonnested models. (If one model is a special case of the other, use standard score/LR tests).
* Logic:
o Even if misspecified, the average log-likelihood consistently estimates the negative of the Kullback-Leibler Information Criterion (KLIC).
o It is legitimate to choose the model with the largest log-likelihood as the "best fit".
o Analogy: Comparing log-likelihoods is similar to comparing $R$-squareds in regression.
* Vuong's Test (1989):
o Setup: Let $f_1(y|x; \theta_1)$ and $f_2(y|x; \theta_2)$ be competing models for $D(y_i|x_i)$.
o Estimators: Let $\hat{\theta}_1$ and $\hat{\theta}_2$ be QMLEs converging to $\theta_1^*$ and $\theta_2^*$.
o Log-Likelihood Difference:
$$(\mathscr{L}_1 - \mathscr{L}_2)/N \xrightarrow{p} E[\log f_1(y_i|x_i; \theta_1^*)] - E[\log f_2(y_i|x_i; \theta_2^*)]$$
o Null Hypothesis ($H_0$): The models are equivalent (fit equally well):
$$E[\ell_{i1}(\theta_1^*)] = E[\ell_{i2}(\theta_2^*)]$$
o Statistic:
$$N^{-1/2} \sum_{i=1}^N [\ell_{i1}(\hat{\theta}_1) - \ell_{i2}(\hat{\theta}_2)] \xrightarrow{d} Normal(0, \omega^2)$$
* Where $\omega^2 = Var[\ell_{i1}(\theta_1^*) - \ell_{i2}(\theta_2^*)]$.
o Implementation:
* Define $d_i = \ell_{i1}(\hat{\theta}_1) - \ell_{i2}(\hat{\theta}_2)$.
* Regress $d_i$ on unity (intercept only).
* The usual $t$ statistic measures whether the mean is different from zero.
* This allows for a simple test of model selection.
* Important Conditions:
o Strictly Nonnested: The log-likelihoods must differ for a nontrivial set of outcomes; otherwise, the variance $\omega^2$ is zero and the test fails.
o Interpretation: Rejection implies one model fits better in the KLIC sense, not that the preferred model is correctly specified.
* Extension to Panel Data (Partial MLE):
o Vuong's approach applies to pooled MLEs if we account for serial dependence.
o Procedure: Let $\hat{d}_{it}$ be the difference in log-likelihoods for unit $i$ at time $t$. Let $\hat{s}_i = \sum_{t=1}^T \hat{d}_{it}$.
o Compute the test statistic using $\hat{s}_i$ (heteroskedasticity and serial correlation robust standard error from pooled regression).
* Alternative: Cox Test (1961, 1962):
o Tests a specified null model against a nonnested alternative.
o Assumes the null model is correct under $H_0$ (unlike Vuong which assumes both may be misspecified).
o Often computationally intractable because it requires estimating the conditional mean difference in log-likelihoods.
13.11.3 Quasi-Maximum Likelihood Estimation in the Linear Exponential Family
* Linear Exponential Family (LEF):
o A class of densities including Normal, Bernoulli, Poisson, and Exponential.
o Log-Likelihood Form:
$$\log f(y|\mu) = a(\mu) + b(y) + y c(\mu)$$
o Property: For these densities, minimizing the KLIC is equivalent to minimizing the squared error between the sample mean and population mean.
* Consistency Result:
o If we specify the conditional mean correctly as $E(y|x) = m(x, \theta)$, the QMLE obtained by maximizing the LEF log-likelihood is consistent for $\theta$.
o Fisher Consistency: The QMLE is consistent provided the mean is correctly specified, regardless of the actual distribution $D(y|x)$.
o Example: Poisson QMLE is consistent for parameters of an exponential mean function even if the data are not Poisson distributed (e.g., they are overdispersed).
* The Score and Hessian:
o Score:
$$s_i(\theta) = \nabla_\theta m(x_i, \theta)' \frac{[y_i - m(x_i, \theta)]}{v(m(x_i, \theta))}$$
* $v(\mu)$ is the variance function associated with the chosen density (e.g., $v(\mu)=\mu$ for Poisson, $v(\mu)=1$ for Normal).
* Note that $E[s_i(\theta_0)|x_i] = 0$ holds as long as the mean $m(x, \theta)$ is correct.
o Expected Hessian (Conditional):
$$A(x_i, \theta_0) = \nabla_\theta m(x_i, \theta_0)' \nabla_\theta m(x_i, \theta_0) / v(m(x_i, \theta_0))$$
* Inference and Variance Assumptions:
o Correct Variance Specification: If $Var(y_i|x_i) = v(m(x_i, \theta_0))$ (the variance implied by the LEF density holds), then the Information Matrix Equality holds. Standard MLE inference is valid.
o GLM Variance Assumption:
$$Var(y_i|x_i) = \sigma^2 v(m(x_i, \theta_0))$$
* Allows for overdispersion ($\sigma^2 > 1$) or underdispersion.
* $\sigma^2$ can be consistently estimated using Pearson residuals:
$$\hat{\sigma}^2 = \frac{1}{N-P} \sum_{i=1}^N \frac{[y_i - m(x_i, \hat{\theta})]^2}{v(m(x_i, \hat{\theta}))}$$
* Under this assumption, $Avar(\hat{\theta}) = \sigma^2 A^{-1}$, and standard errors are just scaled by $\hat{\sigma}$.
o Fully Robust Inference: If the variance is unrestricted, use the sandwich estimator:
$$Avar(\hat{\theta}) = A^{-1} B A^{-1}$$
.
* Relation to Generalized Linear Models (GLM):
o GLM assumes an index structure for the mean $m(x, \theta) = g^{-1}(x\theta)$, where $g(\cdot)$ is the link function.
o The QMLE in the LEF is essentially a GLM estimator.
o There is a close link between QMLE in the LEF and Weighted Nonlinear Least Squares (WNLS). For every QMLE in LEF, there is an asymptotically equivalent WNLS estimator.
Based on your request to avoid duplicating sections and my analysis of the timestamps in your uploaded batch, it appears that Section 13.11.4 (images from 12:20) is the only new material. The other images cover Chapters 5, 6, and earlier parts of 13 which have already been processed.
Here is the reproduction of the new section 13.11.4.
13.11.4 Generalized Estimating Equations for Panel Data
* Overview:
o Extends QMLE in the Linear Exponential Family (LEF) to panel data.
o Approach: Specify conditional mean functions $m_t(x_{it}, \theta)$ and an LEF density, then estimate by maximizing the pooled quasi-likelihood (ignoring time dependence).
o Flexibility: Parameters can change over time (e.g., different intercepts/time dummies in the exponential function).
* Consistency:
o Requires correct specification of the conditional mean for each $t$:
$$E(y_{it} | x_{it}) = m_t(x_{it}, \theta_0), \quad t=1, ..., T$$
o Does not require strict exogeneity of $x_{it}$ (only contemporaneous conditioning).
o The estimator is the Pooled QMLE (or partial QMLE).
* Score and Inference:
o The score for each $t$ has the standard LEF form:
$$s_{it}(\theta) = \nabla_\theta m_t(x_{it}, \theta)' [y_{it} - m_t(x_{it}, \theta)] / v_t(m_t(x_{it}, \theta))$$
o Dynamic Completeness: If the conditional mean is dynamically complete ($E(y_{it}|x_{it}, y_{i,t-1}, ...) = E(y_{it}|x_{it})$), scores are serially uncorrelated.
o Robust Inference: Without dynamic completeness, scores are serially correlated. The appropriate asymptotic variance is the fully robust sandwich estimator:
$$\left( \sum_{i=1}^N \sum_{t=1}^T \nabla_\theta s_{it} \right)^{-1} \left( \sum_{i=1}^N \sum_{t=1}^T \sum_{r=1}^T s_{it} s_{ir}' \right) \left( \sum_{i=1}^N \sum_{t=1}^T \nabla_\theta s_{it} \right)^{-1}$$
* This is the standard "cluster-robust" variance reported by GEE/GLM routines for panel data.
* GLM Variance Assumption:
o If we assume $Var(y_{it}|x_{it}) = \sigma^2 v(m_t(x_{it}, \theta_0))$ (constant dispersion $\sigma^2$ across $t$):
* $\sigma^2$ can be estimated using squared standardized residuals (Pearson residuals).
* Standard errors are scaled by $\hat{\sigma}$.
* Generalized Estimating Equations (GEE):
o Motivation: Improve efficiency over pooled QMLE by explicitly modeling the serial correlation, even if that correlation structure is misspecified.
o Procedure:
1. Specify a "working" correlation matrix $R(\rho)$ (e.g., exchangeable or unstructured).
2. Estimate mean parameters $\hat{\theta}$ by pooled QMLE.
3. Estimate correlation parameters $\hat{\rho}$ using standardized residuals from step 2.
4. Re-estimate $\theta$ using Weighted Multivariate Nonlinear Least Squares (WMNLS) with the estimated weighting matrix.
o Strict Exogeneity Requirement: Unlike pooled QMLE, GEE with a nondiagonal working correlation matrix generally requires strict exogeneity of regressors for consistency.
o Semirobustness: GEE estimators are consistent if the mean is correctly specified (and strict exogeneity holds), even if the working correlation matrix is wrong. However, standard errors must be robust to this misspecification.
Summary:
o Pooled QMLE and GEE are essential for models with unobserved heterogeneity and strictly exogenous regressors.
o They allow inference robust to misspecification of the serial correlation structure.


